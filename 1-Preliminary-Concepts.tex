\chapter{Preliminary Concepts}

\section{Introduction} This section describes basic concepts to establish an intuitive background that will be useful when providing formal definitions.

\begin{lpar}{The syntactic nature of Type Theory.}
    For a mathematician trained in classical theories (like Group Theory or Geometry), the definition of Homotopy Type Theory can seem unusual. Typically, mathematical structures are defined \emph{axiomatically} or \emph{descriptively}: we assume a background universe (usually ZFC Set Theory) and specify a list of properties that a collection of objects must satisfy (e.g., ``A Projective Plane is a set of points and lines such that \dots'').

    However, Type Theory is not a structure \emph{within} mathematics; it is a \emph{foundation for} mathematics. As such, it cannot rely on a pre-existing universe of sets. Instead, it must be defined \textsl{syntactically} or \textsl{generatively}, much like a programming language.

    This approach relies on three components:
    \begin{enumerate}
        \item \textbf{The Alphabet:} A finite set of primitive symbols (e.g., variables $x, y$, separators `$\colon ,\,(,\,)$', and keywords like $\lambda, \Pi$).
        \item \textbf{The Grammar (Syntax):} A set of structural rules that determine which strings of symbols are ``well-formed'' expressions.
        \item \textbf{The Inference Rules (Logic):} A deductive system that defines how valid judgments can be derived from previous ones. For example, the rule for function application allows us to transition from having ``a function $f$'' and ``an input $a$'' to having ``a term $f(a)$''.
    \end{enumerate}

    In this view, mathematical objects are not pre-existing entities that we capture with axioms; they are \textsl{constructions} that we build using the rules. A ``proof'' is not an abstract truth value, but a valid derivation tree within this grammar.

    Therefore, when we ask \textit{What is a type in} HoTT?, the answer is not given by describing its properties in Set Theory, but by specifying the \emph{inference rules} that govern how to construct it, how to construct its elements (introduction), and how to use them (elimination).

    To put this generative process into action, the theory requires a set of \textsl{primitive objects} (or base types) to serve as the \textit{seeds} of the universe. Just as a grammar needs terminal symbols to form sentences, Type Theory postulates specific starting pointsâ€”such as the Empty type ($\fun{0}$), the Unit type ($\fun{1}$), and the Natural Numbers ($\N$).
    
    These primitives are not constructed from simpler pieces; they are introduced by \emph{fiat} through their own specific rules (Formation, Introduction, Elimination, Computation). They act as the \textit{base cases} of the mathematical induction that builds the universe: once these primitives exist, the generative machinery (like $\Sigma$ and $\Pi$ types) can combine them to construct arbitrarily complex structures.
\end{lpar}

\begin{paragraph}{The Problem: Runtime Errors vs.~Mathematical Certainty.}
    Conventional programming languages permit runtime errors that are inconceivable in mathematical practice. A classic example is the ``out-of-bounds'' condition: an instruction like $\fun{v[i]}$ can fail because the type of $\fun i$ (e.g., $\fun{integer}$) is too general to guarantee its value is within the valid domain of the array.

    While software techniques like preconditions, runtime checks, or user-defined types can mitigate this, they do so at the cost of increased complexity or efficiency loss, without ever fully eliminating the problem.

    This class of error is alien to mathematics because the validity of an expression is pre-established. An expression like $v_i$ is only well-formed if the index $i$ is known---explicitly or implicitly---to belong to the appropriate domain. In essence, every term in a mathematical expression carries with it the assumption or proof of its own applicability; the index $i$ is not just an integer, but a pair $(i,p)$ where $p$ justifies that $i$ is within bounds.

    Type Theory seeks to integrate this mathematical property into the programming paradigm itself. However, the traditional approach based on Zermelo-Fraenkel (ZFC) set theory is not the best choice for this.

    The reason is that ZFC operates in a two-layered system: a theory of \textit{objects} (sets) and an \textit{external} system of propositional logic used to make and prove claims \textit{about} those objects. This separation introduces a complexity that scales poorly when building verified software.

    Type Theory offers a more integrated solution by unifying these two concepts. In this paradigm, a \textit{type} embodies both the object and the logic (the proposition). A term of that type is not just a value; it is a \textit{witness} (or proof) that the proposition it represents is true. We also say $a$ is an \textsl{element} of the type~$A$ when $a\colon A$.

    By constructing a term of a specific type, the construction itself \textit{is} the proof that the term satisfies the required properties. This eliminates the problem of uncertain validity, not by checking for it, but by making it logically impossible to express. Since the proof of correctness is inherent in the construction of the terms, the solution is not an added burden that ``scales'' with software complexity---it \textit{is} the paradigm itself.
\end{paragraph}

\section{Basic Notions}
\begin{lpar}{Atoms.}
    Intuitively, \textsl{mathematical expressions} denoting \textit{values} or \textit{quantities} are composable under the grammar defined in a given theory. The building blocks of these expressions are \textsl{atomic}, while the well-formed expressions derived from them are \textsl{compound} (or \textsl{constructed}). For example $\emptyset$ is atomic and $\set\emptyset$ is compound.
\end{lpar}

\begin{lpar}{Judgments.}
    A \textsl{deductive system} is a set of \textsl{rules} that can be applied to derive \textsl{judgments}. For example, in the deductive system of first-order logic (on which Set Theory is based), there is only one kind of judgment: that a given proposition has a proof. Thus, in the deductive system of first-order, if $A$ and $B$ are propositions, then \textit{$A$ has a proof} and \textit{$B$ has a proof} are judgments and we can derive the judgment \textit{$A\wedge B$ has a proof}.

    The foundation of Type Theory is the deductive system. The theory operates fundamentally using a set of established \textit{rules} for deriving judgments. The rules of Type Theory can be grouped into \textit{type formers}. Each type former consists of a way to construct types (possibly making use of previously constructed types), together with rules for the construction and behavior of elements of that type (see \nref{lpar:witnesses}).
\end{lpar}

\begin{lpar}{Metatheory.}
    The conceptual space where theorems about the deductive system are stated and proven is called the \textsl{metatheory}.
\end{lpar}

\begin{lpar}{Witnesses.}\label{lpar:witnesses}
    In Type Theory, the judgment $a\colon A$ is analogous to the notion that $A$ has a proof. This atomic expression is interpreted by saying that $a$ is a \textsl{witness} of the provability of the type $A$. Another analogy with Set Theory links $a\colon A$ with $a\in A$. However, $a\in A$ is a proposition not a judgment, whereas $a\colon A$ is a judgment, not a proposition.

    The type-theoretic perspective treats proofs not merely as communicative means but as first-class mathematical objects in their own right, on par with familiar objects like numbers and functions. Calling the term $a$ a witness (or \textsl{evidence}) highlights that $a$ is a \textit{construction}, consistent with the view of proofs as mathematical objects
    
    The word ``witness'' helps maintain the distinction: \textit{$A$ is provable} is a metatheoretic statement ($A$ is \textsl{inhabited}), while $a$ is the specific internal object providing the justification.
    
    In summary, referring to $a$ as a \textit{proof} is avoided. This emphasizes that $a$ is an object or construction (a witness) within the theory's types, not merely an abstract, metatheoretic declaration of provability.
    
    Note also that in Type Theory, we cannot introduce a variable without specifying its type using an atomic judgment such as $a\colon A$.
\end{lpar}

\begin{lpar}{Equalities.}
    Since in Type Theory propositions are types, this means that equality is a type: for elements $a, b\colon A$ (that is, both $a\colon A$ and $b\colon A$) we have a type $a\eq{A} b$. When $a\eq{A} b$ is inhabited we say that $a$ and $b$ are \textsl{propositionally equal}.

    The second concept of equality is that of \textsl{judgmental}, \textsl{computational}, or \textsl{definitional equality}, denoted by $a\equiv b\colon A$. This means that $a$ and $b$ are syntactically identical or reduce to the same form via the theory's \textsl{computation rules}, which allow automatic rewriting of expressions. For instance, if $f(x)=x^2$, then $f(3)$ and $3^2$ are equal by definition. The idea is that definitional equality is algorithmically decidable (though the algorithm is metatheoretic, not internal to the theory, i.e., is part of the implementation of the type checker in a computer language, not a function defined within the formal system itself).

    The rule that links both concepts of equality establishes that $a\colon A$ and $A\equiv B$ allow us to derive $a\colon B$. In our example, if $p\colon3^2=9$, then $p\colon f(3)=9$.

    We shall use $\defeq$ to define a thing as (judgmentally) equal to another. For instance $f(x)\defeq x^2$.
\end{lpar}

\begin{lpar}{Precedence.}
    Syntactically, the precedence of `$:$' and `$\equiv$' is the lowest in an expression. The reason is that both symbols are used for judgments, which cannot be put together into more complicated statements. Thus, $A\equiv x=y$ means $A\equiv(x=y)$ since $x=y$ is a type and a judgment (like $A\equiv x$) cannot be equal to anything.\footnote{In fact, judgments cannot be terms in any propositional equality.}
\end{lpar}

\begin{lpar}{Assumptions.}\label{lpar:assumptions}
    Judgments may depend on \textsl{assumptions} of the form $x\colon A$. The collection of assumptions is a \textsl{context}, which must be specified by an ordered list because any assumption might depend on a previous assumption in the list. For example $x,y\colon A$ and $p\colon x\eq{A} y$ might form the context for $p^{-1}\colon y\eq{A} x$. When the type involved represents a proposition, assumptions play the role of \textit{hypotheses}.
\end{lpar}

\begin{lpar}{Definitional substitution.}\label{lpar:definitional-substitution}
    Given that $x\equiv a$ is not a type, we cannot have it as an assumption. Instead we \textsl{substitute $a\colon A$ for $x$} and obtain a more specific type or element. Even though not technically an assumption, the language ``assume $x\equiv a$'' will be used to refer to this process of substitution.
\end{lpar}

\begin{lpar}{Principles.}
    Some statements (whether a judgment, axiom, or theorem) that possess high generality and act as fundamental building blocks for reasoning within the theory are called \textsl{principles}. This is not a formal notion, but a pedagogical or communicational one aimed at underlying its importance in the deductive system.

    Substitution (see above), for instance, is usually referred to as the \textit{principle of substitution}. Note that this is a judgmental principle.

    Other statements that correspond to rules, axioms or theorems are broadly called principles.
\end{lpar}

\section{Function Types}

\begin{lpar}{Functions.}
    Given types $A$ and $B$, we can construct the type $A\to B$ of \textsl{functions with domain $A$ and codomain $B$}. We also sometimes refer to functions as \textsl{maps}. Unlike in Set Theory, functions are not defined as functional relations but as a primitive concept in Type Theory. We explain the function type by prescribing what we can do with functions, how to construct them and what equalities they induce. Note also that $f\colon A\to B$ stands for both the traditional notation and the type judgment. In both cases the expression is known as the \textsl{signature} of~$f$.

    Given a function $f\colon A\to B$ and an element of the domain $a\colon A$, we can \textsl{apply} the function to obtain an element of the codomain $B$, denoted $f(a)$ and called the \textsl{value of\/ $f$ at\/ $a$}. It is common in Type Theory to omit the parentheses and denote $f(a)$ simply by $f\,a$.

    \textit{Remark:} Functions are a primitive concept.
\end{lpar}

\begin{lpar}{Binders.}\label{lpar:binders}
    A \textsl{binder} (a.k.a.~\textsl{variable binder}) is an \textit{operator}~$\nabla$ that can be applied to a variable $x$ of type $A$, and an expression $\Phi$ (possibly involving $x$), using the syntax
    \[
        \nabla x\colon A.\,\Phi.
    \]
    We say that $x$ is \textsl{bound} by $\nabla$ inside $\Phi$. A variable is \textsl{free} in an expression if it is not bound by any binder.
\end{lpar}

\begin{lpar}{Function abstraction.}
    There are two ways of defining a function. We define $f\colon A\to B$ by giving an equation
    \[
        f(x)\defeq\Phi
    \]
    where $x$ is a (free) variable and $\Phi$ is an expression which may use $x$. In order for this to be valid, we have to check that $\Phi\colon B$ assuming $x\colon A$ (see \nref{lpar:assumptions}). For example, $f(x)\defeq x^2$ defines a function $f\colon\N\to\N$, and so $f(3)$ is judgmentally equal to $3^2$.

    We can also use the \textsl{$\lambda$-abstraction} binder
    \[
        (\lambda x\colon A.\,\Phi)\colon A\to B.
    \]
    For example, $(\lambda x\colon\N.\,x^2)\colon\N\to\N$ defines the same function as above. Another example is the \textsl{constant function} $(\lambda x\colon A.\,b)\colon A\to B$, where $b\colon B$. This function has the value $b$ at every $a\colon A$.

    \textbf{Note.} The mathematical notation $f(x)=\Phi$, where $\Phi$ is an expression possibly involving $x$, binds the variable $x$ in the scope of $\Phi$. However, unlike other binders such as $\forall$, $\exists$, $\Pi$, $\Sigma$, etc., this one is named after the function. The $\lambda$-notation, \textit{removes} this dependency, emphasizing the binding character of the entire expression. 
\end{lpar}

\begin{lpar}{Implicit notation.}
    Whenever the type $A\to B$ is clear from the context, the $\lambda$-notation can be relaxed to $\lambda x.\,\Phi$. Another way to express the same is $(x\mapsto\Phi)\colon A\to B$.
\end{lpar}

\begin{lpar}{Computation rule.}
    Evaluation at $a$ takes the form
    \[
        (\lambda x.\,\Phi)(a)\equiv\Phi',
    \]
    where $\Phi'$ is obtained from $\Phi$ by substitution of $a$ for $x$ (see \nref{lpar:definitional-substitution}). This rule is also known as \textsl{$\beta$-conversion} or \textsl{$\beta$-reduction}.
\end{lpar}

\begin{lpar}{Uniqueness principle for function types.}\label{lpar:eta-expansion}
    This principle expresses that each function $f\colon A\to B$ is judgmentally equal to its $\lambda$-abstraction, i.e.,
    \[
        f\equiv\lambda x.\,f(x),
    \]
    meaning that $f$ is uniquely determined by its values. This expression is also known as \textsl{$\eta$-conversion} or \textsl{$\eta$-expansion}.

    Note that this is an example of an \textsl{inference rule} (a judgment) that \textit{qualifies} as a principle due to its importance and generality.
\end{lpar}

\begin{lpar}{Bound variables.}\label{lpar:bound-variables}
    Consider the expression
    \[
        f(x)\defeq\lambda y.\,x+y.
    \]
    Here the variable $y$ is \textsl{bound} to the scope of the expression in the \rhs. It is, in other words, \textsl{dummy} in the sense that it can be replaced with another variable, say $z$, without changing the meaning of the expression. In particular,
    \[
        f(x)\equiv\lambda z.\,x+z.
    \]
    This allows us to perform the substitution of $y$ for $x$ in the first expression, by first using the second:
    \[
        f(y)\equiv\lambda z.\,y+z.
    \]
    This change of variable is intended to avoid the accidental \textit{capture} of the substituted variable.
\end{lpar}

\begin{lpar}{Curried form.}\label{lpar:currying}
    If $f\colon A\times B\to C$ is a function, its \textsl{Curried form} is the function $g\colon A\to(B\to C$) defined as $g(a)(b)=f(a,b)$. The equivalence between a function and its Curried form is extended inductively to the case where the domain is $A_1\times\cdots\times A_n$.

    In combination with $\lambda$-abstractions, for $f\colon A\to B\to C$ given by
    \[
        f(x, y) \defeq\Phi
    \]
    where $\Phi\colon C$, assuming $x\colon A$ and $y\colon B$, corresponds to
    \[
        f \defeq\lambda x.\,\lambda y.\,\Phi.
    \]
    From the viewpoint of Logic, Currying is the transformation of
    \[
        A\wedge B\implies C
    \]
    into
    \[
        A\implies(B\implies C).
    \]
\end{lpar}

\section{Dependent Function Types}

\begin{lpar}{Universes.}\label{lpar:universes}
    As every term must have a type, types must have a type as well. To avoid paradoxical situations derived from the global universe, there is a family of universes $\univ_0$, $\univ_1$, $\univ_2,\dots$, such that $\univ_i\colon\univ_{i+1}$ for all $i\ge0$. Thus, $\univ_0$ is the universe of \textsl{small} types, such as $\N$, etc., while $\univ_{i+1}$ is the universe of types in the universe $\univ_i$.\footnote{The indexing here is distinct from the formal type of natural numbers. It is a meta-linguistic device; any finite alphabet containing symbols like `$\univ$' and a successor mark such as `$'$' would suffice, allowing us to write sequences like $\univ\colon\univ'\colon\univ''$ of arbitrary depth.}

    Since there is no universe\/ $\univ_\infty$, the expression $\lambda i\colon\N.\,\univ_i$ does not define a function (otherwise, its codomain would be a global universe). More precisely, no rule permits the definition of a function where the codomain depends on the value of the input in a way that transcends the universe hierarchy.
\end{lpar}

\begin{lpar}{The cumulative property.} 
    The family of universes is \textsl{cumulative}, meaning that $A\colon\univ_{i+1}$ assuming that $A\colon\univ_i$. The following example illustrates the reason behind this rule.
    
    \begin{xmpl}
        Consider a type constructor $\fun{List}$ that forms the type of lists containing elements of a specific type. If we want to create a list of types, we must determine the universe in which this list lives.
        
        \begin{enumerate}[-]
            \item Let $\N\colon\univ_0$ and $\fun{Bool}\colon\univ_0$.
            
            \item We can form the list $[\N, \fun{Bool}]$. The type of this list is $\fun{List}(\univ_0)$.
            
            \item However, since $\univ_0\colon\univ_1$, the type $\fun{List}(\univ_0)$ itself resides in $\univ_1$.
        \end{enumerate}
        Now, suppose we have a more complex type $B$ such that $B\colon\univ_1$. If we wish to construct a list containing both $\N$ and $B$, we face a potential universe mismatch. To include $\N$ in a list of type $\fun{List}(\univ_1)$, we must view $\N$ as an element of $\univ_1$.
        
        Therefore, the construction of the list $[\N,B]$ relies on the cumulative property:
        \[
            \N\colon\univ_0 \implies \N\colon\univ_1.
        \]
        This ensures that both $\N$ and $B$ are valid terms of type $\univ_1$, making the expression $[\N,B]$ well-typed as a term of $\fun{List}(\univ_1)$. \qed
    \end{xmpl}

    \textbf{Note.} To simplify the notation, the subindex $i$ is usually omitted and $\univ$ is used instead of $\univ_i$. When some universe $\univ$ is assumed, its types are also called \textsl{small}.
\end{lpar}

\begin{lpar}{Type Families.}\label{lpar:type-families}
    A \textsl{type family} is a function $B\colon A\to\univ$ whose codomain is a universe.
    
    There are two important examples. The first one is the \textsl{constant} type family defined by
    \[
        (\lambda x.\,B)\colon A\to\univ
    \]
    under the assumption $B\colon\univ$. The second is the family of \textsl{finite} sets\footnote{Formally introduced in Exercise \ref{exr:Fin}.}
    \[
        \fun{Fin}\colon\N\to\univ,
    \]
    where $\fun{Fin}(0)$ is the empty type (it has no elements), $\fun{Fin}(1)$ is a type with exactly one element\footnote{See also \nref{lpar:unit-type}.}, and so on. By its cumulative character (see \nref{lpar:universes}), $\fun{Fin}(n)$ has $n$ elements denoted by $0_n,\dots,(n-1)_n$. The idea behind this definition is to have integers $i$ equipped with a proof that $0\le i<n$.

    The $\fun{Fin}$ function is relevant to define a function like
    \[
        \fun{getElem}\colon\fun{Vect}\,A\, n\to\fun{Fin}(n)\to A,
    \]
    whose second argument is guaranteed to be in the range $\nset[0]{n-1}$.
\end{lpar}

\begin{lpar}{Dependent functions \textrm{(a.k.a.~$\Pi$-types)}.} \label{lpar:Pi-types}
    Given a type $A\colon\univ$ and a family $B\colon A\to\univ$, we may construct the type of \textsl{dependent functions}
    \[
        \prod_{x\colon A}B(x)\colon\univ,
    \]
    by means of the binder $\Pi x.\,B(x)$ (see \nref{lpar:binders}).
    
    This type is a generalization of the function type $A\to B$, which can be recovered by taking the constant family $\lambda x\colon A.\,B$, to produce the definitional equality
    \begin{equation}\label{eq:function=independent-dependent-function}
        \prod_{x\colon A}B\equiv A\to B.
    \end{equation}
    As before, we can apply a dependent function $f\colon\prod_{x\colon A}B(x)$ to an argument $a\colon A$ and obtain an element $f(a)\colon B(a)$. In particular, we have the following computation rule: if $a\colon A$ and $f(x)\defeq\Phi$, then $f(a)\equiv\Phi'$ and $(\lambda x.\,\Phi)(a)\equiv\Phi'$, where $\Phi'$ is obtained by replacing all free occurrences of $x$ in $\Phi$ by $a$, i.e., avoiding dummy variable captures (see \nref{lpar:bound-variables}).

    An example that will become useful later is the dependent function\footnote{Exercise \ref{exr:Fin}.}
    \[
        \fun{fmax}\colon\prod_{n\colon\N}\fun{Fin}(\fun{succ}(n))
    \]
    that selects the last element of each nonempty finite type (see \nref{lpar:type-families})
    \[
        \fun{fmax}(n)\defeq n_{n+1}.
    \]
    \textbf{Note.} When not limited by parentheses, the scope of $\Pi$ extends over the rest of the expression. In particular,
    \[
        \prod_{x\colon A}A\to A\quad\text{stands for}\quad\prod_{x\colon A}(A\to A).
    \]
\end{lpar}

\begin{lpar}{Polymorphic functions.}
    A \textsl{polymorphic} function is one which takes a type as one of its arguments, and then acts on elements of that type (or of other types constructed from it).

    An example is the \textsl{polymorphic identity} function
    \[
        \id\equiv\prod_{A\colon\univ}A\to A,\quad
        \id\defeq\lambda A\colon\univ.\,\lambda x\colon A.\,x,
    \]
    or $\id_A(x)\defeq x$ for short.

    When using the Curried form to define dependent functions with several arguments, the second domain may depend on the first one, and the codomain may depend on both. That is, given $A\colon\univ$ and type families $B\colon A\to\univ$ and $C\colon\prod_{x\colon A}B(x)\to\univ$, we may construct the type
    \[
        \prod_{x\colon A}\;\prod_{y\colon B(x)}C(x,y)
    \]
    of functions with two arguments.
    
    In the case where $B$ is constant and equal to $A$, we may condense the notation and write $\prod_{x,y\colon A}C(x,y)$.

    Consider, for instance, the function
    \[
        \fun{swap}\colon
            \prod_{A,B,C\colon\univ}(A\to B\to C)\to(B\to A\to C)
    \]
    defined as
    \[
        \fun{swap}(A,B,C,f)\defeq\lambda b.\,\lambda a.\,f(a,b)
    \]
    that swaps the order of the arguments and may also be denoted by 
    \[
        \fun{swap}_{A,B,C}(f)(b,a)=f(a,b).
    \]
    Note that given $f\colon\prod_{x\colon A}\prod_{y\colon B(x)}C(x,y)$, we have $f(a,b)\colon C(a,b)$, assuming $a\colon A$ and $b\colon B(a)$.
\end{lpar}

\section{Product Types}
\begin{lpar}{Products.}
    Given types $A,B\colon\univ$ we introduce their \textsl{cartesian product} $A\times B\colon\univ$.
    
    For now, we only declare $A\times B$ as a (new) type, and specify that all pairs $(a,b)$ with $a\colon A$ and $b\colon B$ satisfy $(a,b)\colon A\times B$. In \nref{lpar:products-revisited}, we will further establish the additional rules for defining this new type.
    
    Even though the pairs $(a,b)$, with $a\colon A$ and $b\colon B$,  are of type $A\times B$, the definition of this type makes no attempt at specifying that all the elements of $A\times B$ are pairs (as is the case in Set Theory). By contrast, we will derive this fact as a theorem.    
\end{lpar}

\begin{lpar}{Unit type.}\label{lpar:unit-type}
    The \textsl{unit type} is the nullary product type (i.e., the cartesian product of zero types), denoted by $\fun1\colon\univ$. The \textsl{canonical} element is $\fun\star\colon\fun1$.

    While this definition is consistent with $\fun{Fin}(1)$, a fundamental difference is that the unit type does not rely on $\N$. In particular, it does not depend on the definition of the entire $\fun{Fin}$ family.
\end{lpar}

\begin{lpar}{Type specification.}\label{lpar:type-spec}
    When introducing a new type, we must specify the following
    \begin{enumerate}[i),font=\scshape]
        \item \textit{Formation rules.} Indicate how to form new types. Example: $A\to B$, $\prod_{x\colon A}B(x)$, etc.

        \item \textit{Constructors.} Define how to introduce elements of that type. Example: $f(x)\defeq x^2$, $\lambda x.\,x^2$, for $f\colon A\to B$ and $x\colon A$.

        \item \textit{Eliminators.}\footnote{The term \textit{elimination} refers to the syntactic transformation from premises to conclusion. In formal logic, when comparing the Abstract Syntax Trees (AST) of the judgments, the \textit{main descriptor} (or root node) of the type being eliminated is replaced by the descriptor characteristic of the conclusion's type.} These rules specify how to use (or operate on) elements of the type. Example: function application $f(a)$.

        \item \textit{Computation rules.} Also known as $\beta$-reductions, prescribe how eliminators operate on constructors. Example: $(\lambda x\colon A.\,\Phi)(a)\equiv\Phi'$, where $a\colon A$, and $\Phi'$ is obtained from $\Phi$ by substitution of $a$ for $x$.

        \item \textit{Uniqueness principle$^*$} (optional). Also known as $\eta$-expansions, express how constructors act on eliminators, dually to the computation rule. Example: $f\equiv\lambda x.\,f(x)$.

        Sometimes, the uniqueness principle is propositional, meaning that the element is propositionally equal to another element obtained from other rules for the type. Similarly, some types include \textit{propositional} computation rules.

        In some cases the uniqueness principle indicates that it is not required to specify $f\colon A\to B$ on all elements of type $A$, and that it suffices to specify it on all the constructors of $A$.
    \end{enumerate}
\end{lpar}

\begin{lpar}{Canonical elements.}\label{lpar:canonical-elements}
    While a constructor is the rule that enables the creation of new elements of a type, an element formed by the direct application of such a rule is called \textsl{canonical}. For instance, the constructor for the type $A\times B$ is the rule that creates the pair $(a,b)\colon A\times B$ from $a\colon A$ and $b\colon B$. The pair $(a,b)$ itself is a canonical element of $A\times B$.
\end{lpar}

\begin{lpar}{Products revisited.}\label{lpar:products-revisited}
    Type specification for products works as follows:
    \begin{enumerate}[i),font=\scshape]
        \item \textit{Formation rules.} Given two types $A$ and $B$ we can form $A\times B$. Moreover, given zero types we have the unit type $\fun1$.

        \item \textit{Constructors.} Given $a\colon A$ and $b\colon B$ we have $(a,b)\colon A\times B$. In addition, $\fun\star$ is the unique element of type $\fun1$.

        \item \textit{Eliminators.} If $f\colon A\times B\to C$, the prescription for evaluating $f$ is given by providing a function $g\colon A\to B\to C$ and defining
        \[
            f((a,b)) \defeq g(a)(b),
        \]
        where $a\colon A$ and $b\colon B$.\footnote{This does not assume that all elements with type $A\times B$ are necessarily pairs. It only specifies that function evaluation is defined by how it works on pairs.} Note how this rule \textit{deconstructs} the pair $(a,b)$ into its coordinates, operating in the reverse direction of the constructor.

        \item \textit{Computation rule.} Let us first recall that the universal property of the product is summarized in the following commutative diagram
        \begin{equation}\label{dgm:product}
            \begin{tikzcd}
            C
                    \arrow[rd,dashed,"\exists!f"]
                    \arrow[rrd,"f_1",bend left]
                    \arrow[rdd,"f_2"',bend right]\\
                &A\times B
                    \arrow[r,"\pi_2"]
                    \arrow[d,"\pi_1"']
                &B\\
                &A
            \end{tikzcd}
        \end{equation}
        However, this definition relies on the existence of the projections $\pi_1$ and $\pi_2$. Therefore, we need to define them first. Consider the $\Pi$-type
        \[
            \prod_{C\colon\univ}A\to B\to C.
        \]
        It represents the type of all functions $f$ satisfying $\dom(f)\equiv A\times B$, of which $\pi_1$ and $\pi_2$ are instances. So, we can define the \textsl{recursor} function
        \begin{equation}\label{eq:rec-A-times-B}
            \fun{rec}_{A\times B}\colon
                \prod_{C\colon\univ}(A\to B\to C)
                \to
                A\times B\to C
        \end{equation}
        specified as
        \begin{equation}\label{eq:rec-A-times-B-spec}
            \fun{rec}_{A\times B}(C,g)((a,b)) \defeq g(a)(b),
        \end{equation}
        where we are using the uniqueness principle that establishes that, to specify a function $f\colon A\times B\to C$, it is enough to specify it on the pairs $(a,b)$ (see \nref{lpar:type-spec}~\textsc{v}).

        In a diagram\footnote{Strictly speaking, the domain of the Curried function\/ $g\colon A\to B\to C$ does not appear as a single object in the diagram, so we represent the input context explicitly.}
        \[
            \begin{tikzcd}[row sep=large,column sep=huge]
                a\colon A,\;b\colon B
                    \arrow[r,"\fun{pair}"]
                    \arrow[dr,"g"']
                & A\times B
                    \arrow[d,"\fun{rec}(g)"]\\
                & C
            \end{tikzcd}
        \]
        where\/ $\fun{rec}$ stands for\/ $\fun{rec}_{A\times B}(C)$.
        
        If we introduce the context\/ $\Gamma\defeq (a\colon A,\;b\colon B)$, the diagram becomes
        \[
            \begin{tikzcd}[row sep=large,column sep=huge]
                \Gamma
                    \arrow[r,"{\langle a,b\rangle}"]
                    \arrow[dr,"g"']
                & A\times B
                    \arrow[d, "\fun{rec}(g)"]\\
                & C
            \end{tikzcd}
        \]
        The first diagram is \textit{functional}: it depicts the constructor\/ $\fun{pair}$ combining inputs to form the product, and how\/ $\fun{rec}$ factors the Curried function\/ $g$. The second is \textit{categorical}: it treats the inputs as a context object\/ $\Gamma$, where the arrow\/ $\langle a,b\rangle$ represents the specific term (substitution) in that context mapping to the product.
        
        We can derive the projections as
        \begin{align*}
            \pi_1 &\defeq \fun{rec}_{A\times B}
                (A.\,\lambda a.\,\lambda b.\,a)\\
            \pi_2 &\defeq \fun{rec}_{A\times B}
                (B.\,\lambda a.\,\lambda b.\,b),
        \end{align*}
        which satisfy $\pi_1((a,b))\equiv a$ and $\pi_2((a,b))\equiv b$, and we can consider them the computation rules associated with the product.

        More generally, the function $\fun{rec}_{A\times B}$, called the \textsl{recursor} for product types, allows us to define functions of type $A\times B\to C$, a property referred to as the \textsl{recursion principle}.

        Note that the judgment
        \[
            \fun{rec}_{A\times B}(C,g)
                \equiv \lambda x.\,g(\pi_1(x))(\pi_2(x))
        \]
        shows that $\fun{rec}$ is also derivable from the projections.

        {\footnotesize
        The viewpoint under which $\fun{rec}$ is introduced is not the one expressed in the universal property of the product \eqref{dgm:product}, but the fact that the functors $F_B(A)=A\times B$ and $G_B(C)=\Hom(B,C)$ satisfy $F_B\dashv G_B$, i.e., $\Hom(F_B(A),C)\cong\Hom(A,G_B(C))$. In this regard, the universal property leads us to view the product as a function codomain, while the adjoint relation leads us to consider it as a domain.}

        The version of $\fun{rec}$ for the empty product type $\fun1$ (where $A$ and $B$ are missing) is
        \[
            \fun{rec}_{\fun1}\colon\prod_{C\colon\univ}C\to\fun1\to C
        \]
        specified as
        \[
            \fun{rec}_{\fun1}(C,c)\defeq\lambda s.\,c.
        \]
        Since every $g\colon\fun1\to C$ is equivalent to choosing one element $c\colon C$, we have
        \[
            \fun{rec}_{\fun1}(C,c)(\fun\star)\equiv c.
        \]
    \end{enumerate}
\end{lpar}

\begin{lpar}{Recursion significance.}
    While projections and recursion are mutually definable for product types, this equivalence does not hold in general. As we will see, recursion is the more fundamental concept; in fact, it is a primitive of Type Theory. For instance, even though $\fun{rec}_{\fun1}$ is well-defined, the Unit type (the empty product) has no projections.

    In colloquial terms, while projections merely extract the parts so that we may operate on them, recursion asks what calculation we wish to perform \textsl{using} the parts, and then carries it out.

    In Object-Oriented Programming, recursion is mediated by encapsulation: an object exposes methods that recursively delegate work to the methods of its constituent parts. In Structured Programming, by contrast, processing operates directly on the data stored in record fields rather than through behavior exported by the data itself.

    Note also that $\fun{rec}$ implements \textsl{Uncurrying} [cf.~\nref{lpar:currying}], namely 
    \[
        \Hom(A,B\to C)\to\Hom(A\times B,C).
    \]
    In Logic, this corresponds to the transformation 
    \[
        A\implies(B\implies C)
        \quad\text{into}\quad
        A \wedge B\implies C.
    \]

    \textbf{Note.} More formally, the definition of every new type $T\colon\univ$ includes a \textit{recursion map}, usually denoted by $\fun{rec}_T$. This map generates functions of type $T\to C$ for any result type $C\colon\univ$, based on values specified for the canonical elements of~$T$.
    
    For example, $\fun{rec}_{A\times B}$ assigns to every type $C\colon\univ$ a function $A\times B\to C$ for each curried function $g\colon A\to B\to C$, as indicated in~\eqref{eq:rec-A-times-B}, which satisfies the computation rule~\eqref{eq:rec-A-times-B-spec}.
    
    As we shall see [cf.~\nref{lpar:induction}], an analogous mechanism must be provided for dependent functions with domain $T$; this is the role of the \textit{induction map}.

\end{lpar}

\begin{lpar}{Identity types.}
    Given a type $A$ and terms $x,y\colon A$ we can form the type $x \eq{A}  y$. If the type $x\eq{A} y$ is inhabited (has a term), the equality is true. If the type $x\eq{A} y$ is empty (has no terms), the equality is false.
    
    The type for equality is called the \textsl{identity type}, written $\Id_A(x,y)$ or $x\eq{A} y$. So, $\Id_A(x,y)$ represents the \textit{type of all proofs that\/ $x$ equals\/ $y$}.
    
    The \textit{unique} canonical term inhabiting the identity type\/ $\Id_A(x,x)$ is\/ $\fun{refl}_x$. Denoted as\/ $\fun{refl}_x : \Id_A(x,x)$, this term witnesses that\/ $x$ is equal to itself.
    
    The fact that $\Id_A(x,x)$ has a unique canonical witness is the key to the identity type's elimination rule (see \nref{lpar:induction}), which states that to prove something for all equalities $p\colon x\eq{A} y$, we only need to prove it for the case $p\equiv\fun{refl}_x$. Note however that this does not mean that $\fun{refl}_x$ is the only element of $\Id(x,x)$; in fact, there are cases where there are infinitely many, as well as cases where there is only one.
\end{lpar}

\begin{lpar}{Dependent functions over product.}\label{lpar:Pi-types-over-product}
    According to the specification of $\Pi$-types, given $A,B\colon\univ$ and a type family $C\colon A\times B\to\univ$, a dependent function
    \[
        f\colon\prod_{z\colon A\times B}C(z)
    \]
    is defined by means of a function 
    \[
        g\colon\prod_{x\colon A}\;\prod_{y\colon B}C((x,y))
    \]
    as
    \[
        f((x,y))\defeq g(x)(y).
    \]
    Thus, $f$ is well defined on (any element of) $A\times B$ as soon as its values are specified in the \textsl{canonical} elements of the product, i.e., the pairs.
    
    Since, given $(a,b)\colon A\times B$, we have (see \nref{lpar:products-revisited})
    \[
        \pi_1((a,b))\equiv a\quad\text{and}\quad
        \pi_2((a,b))\equiv b,
    \]
    we deduce that
    \[
        \bigl(\pi_1((a,b)),\pi_2((a,b))\bigr)\equiv(a,b).
    \]
    In consequence, the types
    \[
        (a,b)\eq{A\times B}(a,b)\quad\text{and}\quad
        \bigl(\pi_1((a,b)),\pi_2((a,b))\bigr)\eq{A\times B}(a,b)
    \]
    are judgmentally equal. Since $\fun{ref}_{A\times B}$ inhabits the former, it also inhabits the latter, i.e.,
    \[
        \fun{refl}_{(a,b)}\colon
            (\pi_1((a,b)),\pi_2((a,b)))\eq{A\times B}(a,b)
    \]
    Given that to construct a witness for the uniqueness principle
    \[
        \fun{uniq}_{A\times B}\colon\prod_{x\colon A\times B}
            (\pi_1(x),\pi_2(x))\eq{A\times B}x.
    \]
    it suffices to specify it on canonical elements, the definition
    \[
        \fun{uniq}_{A\times B}((a,b))\defeq\fun{refl}_{(a,b)}
    \]
    meets this requirement.
    
    \textbf{Note.} This function $\fun{uniq}_{A\times B}$ is a witness (formal proof) of the uniqueness principle for products (see \nref{lpar:eta-expansion}). As we will see in \nref{lpar:induction} below, this fulfills the claim from \nref{lpar:products-revisited} that all terms of type $A \times B$ are (propositionally) pairs.
    
    In the case of $\fun1$, we have
    \[
        \fun{uniq}_{\fun1}\colon\prod_{x\colon\fun1}
            x\eq{\fun1}\fun\star
    \]
    with
    \[
        \fun{uniq}_{\fun1}(\fun\star)\defeq\fun{refl}_{\fun\star},
    \]
    which shows that $\fun\ast$ is the unique element of $\fun1$.
\end{lpar}

\begin{lpar}{Induction.}\label{lpar:induction}
    The \textit{principle} by which defining a function on a product $A\times B$ amounts to specifying its value on each pair, can be formalized by introducing the induction map
    \[
        \fun{ind}_{A\times B}\colon
            \prod_{C\colon A\times B\to\univ}
                \Bigl(\prod_{x\colon A}\;\prod_{y\colon B} C(x,y)\Bigr)
            \to
            \prod_{z\colon A\times B} C(z),
    \]
    given by
    \[
        \fun{ind}_{A\times B}(C)(g)\bigl((a,b)\bigr)\defeq g(a)(b),
    \]
    or, in abbreviated form,
    \[
        \fun{ind}_{A\times B}(C,g,(a,b))\defeq g(a)(b).
    \]
    \textbf{Interpretation.} The $\fun{ind}_{A\times B}$ function transforms every proof that a proposition $C$ is true for all canonical pairs $(x,y)$ into a proof of $C(z)$ for any element $z\colon A\times B$.
    
    When the family $C$ is constant we obtain
    \[
        \prod_{x\colon A}\;\prod_{y\colon B}C((x,y))
            \equiv A\to B\to C
        \quad\text{and}\quad
        \prod_{z\colon A\times B}C(z)\equiv A\times B\to C,
    \]
    Hence, $\fun{ind}_{A\times B}$ becomes $\fun{rec}_{A\times B}$. Consequently, induction is the (\textsl{dependent}) \textsl{eliminator} and recursion the \textsl{non-dependent eliminator}.

    In the case of the unit type (see \nref{lpar:unit-type}), where types $A$ and $B$ are both missing, the eliminator is
    \begin{equation}\label{eq:ind1}
        \fun{ind}_{\fun1}\colon\prod_{C\colon\fun1\to\univ}
            \Bigl(C(\fun\star)\to\prod_{x\colon\fun1}C(x)\Bigr)
    \end{equation}
    specified by
    \[
        \fun{ind}_{\fun1}(C,g,\fun\star)\defeq g,\footnote{A more precise notation is $\fun{ind}_{\fun1}(C)(g)(\fun\star)\defeq g$.}
    \]
    
    \textbf{Interpretation.} Given $C\colon\fun1\to\univ$ and a proof $g$ that $C(\fun\star)$ is true, the resulting term $\fun{ind}_{\fun1}(C,g)$ ---i.e., $\fun{ind}_{\fun1}(C)(g)$--- is a dependent function $f\colon \prod_{x\colon\fun1}C(x)$ that serves as a proof for $C(x)$ for all $x\colon\fun1$.
        
    \needspace{2\baselineskip}
    In summary, we have:
    \begin{enumerate}[label=\roman*), font=\scshape]
        \item \textit{Type:} There is a type called $\fun1$.
        \item \textit{Constructor:} There is a term $\fun\star\colon\fun1$.
        \item \textit{Eliminator:} There is a function $\fun{ind}_{\fun1}$ with type signature \eqref{eq:ind1}.
        \item \textit{Computation Rule:} $\fun{ind}_{\fun1}(C,g,\fun\star)\defeq g$.
    \end{enumerate}

    \medskip
    
    If for $C\colon\fun1\to\univ$ we take
    \[
        C\defeq\lambda x.\,(x\eq{\fun1}\fun\star)
    \]
    and for $g\colon C(\fun\star)$ we take $\fun{refl}_{\fun\star}$, then the result is
    \[
        \fun{ind}_{\fun1}((\lambda x.
            \,x\eq{\fun1}\fun\star)
            ,\fun{refl}_{\fun\star}
            ,\fun\star)\equiv\fun{refl}_{\fun\star},
    \]
    which is judgmentally equal to $\fun{uniq}_{\fun1}(\fun\star)$ (see \nref{lpar:Pi-types-over-product}). Thus,
    \begin{equation}\label{eq:uniq1-from-ind1}
        \fun{ind}_{\fun1}\bigl(\lambda x.\,x \eq{\fun1} \fun\star,\;\fun{refl}_{\fun\star}\bigr)
        \equiv\fun{uniq}_{\fun1}.
    \end{equation}
    Note that $\fun{ind}_{\fun1}$ is strictly more general than $\fun{uniq}_{\fun1}$, since it is defined for an arbitrary family $C\colon\fun1\to\univ$. Equation~\eqref{eq:uniq1-from-ind1} shows that $\fun{uniq}_{\fun1}$ is simply a specific instance of $\fun{ind}_{\fun1}$. Thus the fact that $\fun1$ is a singleton type is subsumed by the more general induction principle.
\end{lpar}

\section{Dependent Pair Types}

\begin{lpar}{Dependent pairs \textrm{(a.k.a.~$\Sigma$-types)}.}
    \label{lpar:Sigma-types}
    In Set Theory the \textsl{disjoint union} of a family $(A_i)_{i\in I}$ of sets is defined as
    \[
        \bigsqcup_{i\in I}\set i\times A_i,
    \]
    where the indexes in the first coordinate ensure that
    \[
        \set i\times A_i\cap \set j\times A_j=\emptyset
    \]
    whenever $i\ne j$, even though $A_i$ and $A_j$ might overlap (or coincide).

    The equivalent notion in Type Theory is the \textsl{dependent pair type} (a.k.a.~\textsl{$\Sigma$-type}), which highlights the dependence of the second component on the first one. More precisely, given a type $A\colon\univ$ and a type family $B\colon A\to\univ$, the binder $\Sigma$ is used to define the \textsl{dependent pair} type
    \[
        \sum_{x\colon A}B(x)\colon\univ.
    \]
    As it is the case with $\lambda$ and $\Pi$, this binder also scopes over the rest of the expression, e.g.,
    \[
        \sum_{x\colon A}B(x)\to C\quad\text{stands for}\quad
            \sum_{x\colon A}(B(x)\to C).
    \]
    The constructor takes $a\colon A$ and $b\colon B(a)$ and produces
    \[
        (a,b)\colon \sum_{x\colon A}B(x).
    \]
    In this regard, $\Sigma$-types resemble the existential binder $\exists$ of Set Theory in that its elements $(a,b)$ consist of a first component $a$ and a second $b$, which is a proof of $B(a)$. In particular, in the case where $B$ is constant, we have
    \[
        \sum_{x\colon A}B\equiv A\times B.
    \]
\end{lpar}

\begin{lpar}{Generalized Currying.}\label{lpar:generalized-currying}
    Recall from  \nref{lpar:currying} that standard Currying establishes an equivalence
    \[
        (A\times B)\to C \iff A\to B\to C.
    \]
    In the context of dependent types, the  $\Sigma$-type acts as the generalization of the Cartesian product (see  \nref{lpar:Sigma-types}).
    
    Consequently, the Currying principle extends naturally to this setting: mapping out of a dependent sum is equivalent to mapping out of the components sequentially.
    
    Specifically, a function
    \[
        f\colon\Bigl(\sum_{x\colon A}B(x)\Bigr)\to C
    \]
    is equivalent to a dependent function
    \[
        \hat f\colon\prod_{x\colon A}B(x)\to C,
    \]
    where the scope of the binder $\Pi$ includes $C$.

    To create a nondependent function
    \[
        f\colon\Bigl(\sum_{x\colon A}B(x)\Bigr)\to C
    \]
    we have to provide a function
    \[
        g\colon\prod_{x\colon A}B(x)\to C
    \]
    and specify
    \[
        f((a,b)) \defeq g(a)(b).
    \]
\end{lpar}

\begin{lpar}{Induction in $\Sigma$-types.}\label{lpar:Sigma-induction}
    The first projection of a $\Sigma$-type can be defined as
    \[
        \pi_1\colon\Bigl(\sum_{x\colon A}B(x)\Bigr)
            \to A
    \]
    with specification
    \[
        \pi_1((a,b)) \defeq a,
    \]
    which, with the notation of \nref{lpar:Sigma-types}, corresponds to
    \[
        g(a)(b) \defeq a.
    \]
    However, the second projection is the dependent function
    \begin{equation}\label{eq:Sigma-pi-2}
        \pi_2\colon\prod_{p\colon\sum_{x\colon A}B(x)}B(\pi_1(p))
    \end{equation}
    specified as
    \[
        \pi_2((a,b)) \defeq b.
    \]
    More generally, we need the \textsl{induction principle} of $\Sigma$-types, which says that, given a type family
    \[
        C\colon\sum_{x\colon A}B(x)\to\univ,
    \]
    to define a dependent function
    \[
        f\colon\prod_{p\colon\sum_{x\colon A}B(x)}\!\!\!\!\!C(p)
    \]
    we must provide a function
    \[
        g\colon\prod_{a\colon A}\;\prod_{b\colon B(a)}C((a,b))
    \]
    and specify
    \[
        f((a,b)) \defeq g(a)(b).
    \]
    Thus, in the case where $C(p) \defeq B(\pi_1(p))$, we can take
    \[
        g(a)(b) \defeq b
    \]
    for $(a,b)\colon\sum_{x\colon A}B(x)$, and obtain \eqref{eq:Sigma-pi-2}. This is valid because
    \[
        C((a,b))\equiv B(\pi_1((a,b))\equiv B(a)
    \]
    and $b\colon B(a)$.
\end{lpar}

\begin{lpar}{Recursion in $\Sigma$-types.}\label{lpar:Sigma-type-rec-and-ind}
    Consider the recursion
    \[
        \fun{rec}_{\sum_{x\colon A} B(x)}\colon
            \prod_{C\colon\univ}\Bigl(\prod_{x\colon A}
                B(x)\to C\Bigr)
                \to\Bigl(\sum_{x\colon A}B(x)\Bigr)
                \to C
    \]
    specified as
    \begin{equation}\label{eq:Sigma-type-rec-evaluation}
        \fun{rec}_{\sum_{x\colon A}B(x)}(C,g,(a,b))\defeq g(a)(b),
    \end{equation}
    This can be represented by a collection of diagrams that commute for all $a\colon A$ 
    \[
    \begin{tikzcd}
        B(a)
                \arrow[r,"\iota_a"]
                \arrow[dr,"g(a)"']
            &{\sum_{x\colon A}B(x)}
                \arrow[d,dashed,"{\exists!\,\fun{rec}_{\sum_xB(x)}(C,g,a)}"]\\
            &C
    \end{tikzcd}
    \]
    where $\iota_a\defeq\lambda y\colon B(a).\,(a,y)$.
    
    For the induction principle
    \[
        \fun{ind}_{\sum_{x\colon A}B(x)}\colon
            \prod_{C\colon\bigl(\sum_{x\colon A}B(x)\bigr)
                \to\univ}
                    \Bigl(
                        \prod_{a\colon A}\;\prod_{b\colon B(a)}
                            \!\!\!C((a,b))
                    \Bigr)
                    \to\prod_{p\colon\sum_{x\colon A}B(x)}\!\!\!\!\!C(p)
    \]
    where
    \[
        \fun{ind}_{\sum_{x\colon A}B(x)}
            (C,g,(a,b))\defeq g(a)(b).
    \]
    Note that when $C$ is constant $\fun{ind}$ reduces to $\fun{rec}$.

    \textbf{Interpretation.} To prove $C(p)$ for every $p\colon\sum_{x\colon A}B(x)$ it suffices to show a proof of $C((a,b))$ for every canonical pair $(a,b)$.
\end{lpar}

\begin{lpar}{Relations.}\label{lpar:relations}
    Let $A$ and $B$ be types, $a\colon A$ and $R\colon A\to B\to\univ$. The type
    \[
        \sum_{y\colon B}R(a,y)
    \]
    represents the fiber at $a$ of the relation $R$. Its elements are pairs $(b,\omega)$ where $b\colon B$ and $\omega\colon R(a,b)$ is a witness that $b$ is related to $a$.

    For instance, let $R(2,n)$ stand for the type representing the relation ``$2\mid n$'', where $n\colon\N$. The elements of the type $\sum_{n\colon\N}R(2,n)$ represent the even numbers paired with the evidence of their evenness. Thus, if $\omega$ is a witness that $2\cdot3\eq\N6$, we have
    \[
        (6,\omega)\colon\sum_{n\colon\N}R(2,n).
    \]
    However, there is no term $\omega$ for which $(3,\omega)\colon\sum_{n\colon\N}R(2,n)$, because the type $R(2,3)$ is uninhabited (there is no proof of $2\mid 3$).    
\end{lpar}

\begin{lpar}{Type-theoretic Axiom of Choice.}\label{lpar:axiom-of-choice}
    Now consider the following principle, where $A$ and $B$ are types and $R\colon A\to B\to\univ$:
    \[
        \fun{ac}\colon\Bigl(\prod_{x\colon A}\;\sum_{y\colon B}
            R(x,y)\Bigr)\to\Bigl(\sum_{f\colon A\to B}\;\prod_{x\colon A}R(x,f(x))\Bigr).
    \]
    To specify it, we assume a witness $g$ for the domain:
    \[
        g\colon \prod_{x\colon A}\;\sum_{y\colon B}R(x,y).
    \]
    For $x\colon A$, the term $g(x)$ is a pair $(y,\omega)$ where $y\colon B$ and $\omega\colon R(x,y)$. We can therefore define the \textit{choice function\/} $f\colon A\to B$ as
    \[
        f\defeq\lambda x.\,\pi_1(g(x)).
    \]
    Since $\pi_2(g(x))\colon R(x,\pi_1(g(x)))$, we obtain $\pi_2(g(x))\colon R(x,f(x))$. In addition,
    \[
        \omega\defeq\lambda x.\,\pi_2(g(x)).
    \]
    Finally, we specify
    \begin{align*}
        \fun{ac}&\defeq
                \bigl(
                \overbrace{\lambda x.\,\pi_1(g(x))}^{f\equiv},\,
                \overbrace{\lambda x.\,\pi_2(g(x))}^{\omega\equiv}
                \bigr).
    \end{align*}
    
    \textbf{Note.} Even though both the conventional existence binder $\exists\,y\,R(x,y)$ and the dependent pair type binder $\sum_yR(x,y)$ express the existence of an element~$y$, their semantics differ. While the former predicates the mere existence of $y$, the latter identifies pairs $(y,w)$, where $w$ \textit{proves\/} $R(x,y)$. Thus, in Type Theory, when we evaluate the antecedent function $g$ at $x$, the result $g(x)\equiv(y,w)$ already contains the element $y$ whose existence is claimed. Consequently, the \textit{choice} is trivial in this setting. This explains why, even though the translation to Logic reads as the classical Axiom of Choice, the Type Theoretic version is a theorem whose proof is a direct consequence of the definitions.
\end{lpar}

\begin{lpar}{Mathematical structures.}
    Many mathematical structures are compositions of the simplest one, called magma, that consists of a set $A$ and an operation $m\colon A\times A\to A$. In Type Theory notation the pair $(A,m)$ is an element of a dependent pair type
    \[
        M \defeq\sum_{A\colon\univ}A\to A\to A.
    \]
    In more specific cases, such as pointed magmas, where a distinguished element $e$ must be identified, we would have,
    \[
        PM \defeq\sum_{A\colon\univ}(A\to A\to A)\times A
    \]
    and, consequently, we will have expressions like $(A,(m,e))\colon PM$. By abuse of notation, however, we will sometimes simply say \textit{let $A$ be a magma/group/etc.} as it is usually expressed in the regular mathematical jargon. Similarly, instead of $(A,(m,e))$ we will write $(A,m,e)$ to minimize the number of parentheses.
\end{lpar}

\section{Coproduct Types}
    \begin{lpar}{Coproducts.} Given types $A,B\colon\univ$ their \textsl{coproduct} is $A+B\colon\univ$, a type that corresponds to the \textit{disjoint union} in Set Theory. Its constructors consist of two maps
    \begin{align*}
        \fun{inl}\colon A&\to A+B
            &(\textsl{left injection})\\
        \fun{inr}\colon B&\to A+B
            &(\textsl{right injection})
    \end{align*}
    There is also a nullary version, the \textsl{empty coproduct} (i.e., the coproduct of no types), denoted by $\fun0\colon\univ$, which has no constructors.

    Given\/ $C\colon\univ$, to define a function\/ $f\colon A+B\to C$ we have to provide two maps, as illustrated in the following diagram
    \[
        \begin{tikzcd}
                &B
                    \arrow[d,"\fun{inr}"']
                    \arrow[rdd,"g_1", bend left]\\
            A
                    \arrow[r,"\fun{inl}"]
                    \arrow[rrd,"g_0"', bend right]
                &A+B
                    \arrow[rd,"f",dashed]&\\
                &&C
        \end{tikzcd}
    \]
    This means that to define\/ $f$ we need to specify\/ $g_0$ and\/ $g_1$, which will satisfy
    \[
        f(\fun{inl}(a)) \equiv g_0(a)\quad\text{and}\quad
        f(\fun{inr}(b)) \equiv g_1(b),
    \]
    for\/ $a\colon A$ and\/ $b\colon B$.

    In the case of the null coproduct, which is known as the \textsl{empty type}, given $C\colon\univ$ there is a unique function $f\colon\fun0\to C$. Since the type $\fun0$ has no elements, this function has no further specification.
\end{lpar}

\begin{lpar}{Coproduct recursion.}\label{lpar:coproduct-recursion}
    Consistent with the way functions with domain in a coproduct are defined, we introduce the recursor
    \[
    \fun{rec}_{A+B} \colon \prod_{C\colon\univ}
        (A \to C) \to (B \to C) \to A + B \to C
    \]
    with specification
    \begin{align*}
        \fun{rec}_{A+B}(C, g_0, g_1, \fun{inl}(a)) &\equiv g_0(a),\\
        \fun{rec}_{A+B}(C, g_0, g_1, \fun{inr}(b)) &\equiv g_1(b).
    \end{align*}
    We can also express this recursion with the definitionally commutative diagram
    \[
        \begin{tikzcd}[row sep=large, column sep=large]
            A
                    \arrow[r,"\fun{inl}"]
                    \arrow[dr,"g_0"']
                &A+B
                    \arrow[d,"\fun{rec}" description]
                &B
                    \arrow[l,"\fun{inr}"']
                    \arrow[dl, "g_1"] \\
                & C &
        \end{tikzcd}
    \]
    where $\fun{rec}\defeq\fun{rec}_{A+B}(C,g_0,g_1)$.

    Additionally,
    \[
        \fun{rec}_{\fun0}\colon\prod_{C\colon\univ}\fun0\to C
    \]
    constructs the unique function $\fun0\to C$ for every $C\colon\univ$.
\end{lpar}

\begin{lpar}{Coproduct induction.}\label{lpar:coproduct-induction}
    Given a type family $C\colon(A+B)\to\univ$, to define a dependent function
    \[
        f\colon\prod_{s\colon A+B}C(s)
    \]
    we have to provide two dependent functions
    \begin{align*}
        g_0 &\colon \prod_{x\colon A}C(\fun{inl}(x)),\\
        g_1 &\colon \prod_{y\colon B}C(\fun{inr}(y))
    \end{align*}
    and specify, for $a\colon A$ and $b\colon B$,
    \[
        f(\fun{inl}(a))\equiv g_0(a)\quad\text{and}\quad
        f(\fun{inr}(b))\equiv g_1(b).
    \]
    As a result, we obtain the \textsl{induction principle for coproducts}
    \[
        \fun{ind}_{A+B}\colon\prod_{C\colon(A+B)\to\univ}\;
            \prod_{x\colon A}C(\fun{inl}(x))
            \to
            \prod_{y\colon B}C(\fun{inr}(y))
            \to
            \prod_{s\colon A+B}C(s).
    \]
    In the case of the empty type $\fun0$ (the nullary coproduct), there are no constructors to handle. Thus, the induction principle for the empty type (a.k.a.~eliminator) reduces to
    \[
        \fun{ind}_{\fun0}\colon\prod_{C\colon\fun0\to\univ}\;
            \prod_{s\colon\fun0}C(s).
    \]
    This function asserts that if we assume an element of the empty type, we can derive any conclusion $C(s)$ (the principle of \textsl{ex falso quodlibet}).\footnote{In classical logic, intuitionistic logic, and similar logical systems, the \textsl{principle of explosion} is the law according to which any statement can be proven from a contradiction. That is, from a contradiction, any proposition (including its negation) can be inferred; this is known as deductive \textsl{explosion}. Note however the existence of the field of Inconsistent Mathematics, a theory that explains and studies paradoxes and paraconsistency.}
\end{lpar}

\section{Boolean Type}

\begin{lpar}{The type of booleans.}\label{lpar:boolean-type}
    The \textsl{boolean type} is denoted $\fun2\colon\univ$. Its constructors yield $0_{\fun2}$ and $1_{\fun2}$. To derive a function $f\colon\fun2\to C$ we have to specify two elements $c_0,c_1\colon C$ and declare
    \[
        f(0_{\fun2}) \defeq c_0
        \quad\text{and}\quad
        f(1_{\fun2}) \defeq c_1.
    \]
    Therefore, the recursor
    \begin{equation}\label{eq:rec2}
        \fun{rec}_{\fun2}\colon\prod_{C\colon\univ}C\to C\to\fun2\to C,
    \end{equation}
    is specified by
    \begin{align*}
        \fun{rec}_{\fun2}(C,c_0,c_1,0_{\fun2})&\equiv c_0\\
        \fun{rec}_{\fun2}(C,c_0,c_1,1_{\fun2})&\equiv c_1,
    \end{align*}
    which means that $f\colon\fun2\to C$ is defined by a pair of elements of type $C$. We can capture this in a definitionally commutative diagram
    \[
        \begin{tikzcd}[row sep=0.8cm,column sep=2.0cm]
            \fun1
                    \arrow[dr,"c_0"]
                    \arrow[d,"\iota_{0_{\fun2}}"']\\
            \fun2
                    \arrow[r,"\fun{rec}\quad"]
                &C\\
            \fun1
                    \arrow[ur,"c_1"']
                    \arrow[u,"\iota_{1_{\fun2}}"]
        \end{tikzcd}
    \]
    where $\fun{rec}\defeq\fun{rec}_{\fun2}(C,c_0,c_1)$.

    In the case of dependent functions, given a type family $C\colon\fun2\to\univ$, we define
    \[
        f\colon\prod_{b\colon\fun2}C(b)
    \]
    specifying $c_0\colon C(0_{\fun2})$ and $c_1\colon C(1_{\fun2})$ and setting
    \[
        f(0_{\fun2})\defeq c_0
        \quad\text{and}\quad
        f(1_{\fun2})\defeq c_1.
    \]
    For a dependent function
    \[
        f\colon\prod_{b\colon\fun2}C(b),
    \]
    where $C\colon\fun2\to\univ$, we have
    \[
        \fun{ind}_{\fun2}\colon\prod_{C\colon\univ}\;
            C(0_{\fun2})\to C(1_{\fun2})
            \to\fun2
            \to\prod_{b\colon\fun2}C(b)
    \]
    along with
    \begin{align*}
        \fun{ind}_{\fun2}(C,c_0,c_1,0_{\fun2})&\defeq c_0,\\
        \fun{ind}_{\fun2}(C,c_0,c_1,0_{\fun2})&\defeq c_1.
    \end{align*}
\end{lpar}

\begin{lpar}{Exactly two booleans.}
    Here we will use $\fun{ind}_{\fun2}$ to prove that the boolean type $\fun2$ has exactly two elements, namely, $0_{\fun2}$ and $1_{\fun2}$.

    To see this we use the type family
    \[
        C\colon\fun2\to\univ,\quad
            C\defeq\lambda x.\,(x\eq{\fun2}0_{\fun2})+(x\eq{\fun2}1_{\fun2}).
    \]
    Now, evaluation at the constructors yields
    \begin{align*}
        C(0_{\fun2})&\equiv
            (0_{\fun2}\eq{\fun2}0_{\fun2})
                +(0_{\fun2}\eq{\fun2}1_{\fun2}),\\
        C(1_{\fun2})&\equiv
            (1_{\fun2}\eq{\fun2}0_{\fun2})
                +(1_{\fun2}\eq{\fun2}1_{\fun2}).
    \end{align*}
    Therefore, we have valid witnesses
    \[
        \fun{inl}(\fun{refl}_{0_{\fun2}})\colon C(0_{\fun2})
        \quad\text{and}\quad
        \fun{inr}(\fun{refl}_{1_{\fun2}})\colon C(1_{\fun2}).
    \]
    And so we obtain the proof:
    \[
        \fun{ind}_{\fun2}\bigl(C,
            \fun{inl}(\fun{refl}_{0_{\fun2}}),
            \fun{inr}(\fun{refl}_{1_{\fun2}})\bigr)
            \colon\prod_{x\colon\fun2}
                (x\eq{\fun2}0_{\fun2})+(x\eq{\fun2}1_{\fun2}).
    \]
\end{lpar}

\begin{lpar}{Coproducts as Indexed Disjoint Unions.}\label{lpar:coproduct-as-disjoint-union}
    For $A,B\colon\univ$ consider the recursor $\fun{rec}_{\fun2}$ as defined in \eqref{eq:rec2} with $C\defeq\univ$ and 
    \begin{align*}
        f\colon\fun2&\to\univ\\
        0_{\fun2}&\mapsto A,\\
        1_{\fun2}&\mapsto B.
    \end{align*}
    Thus,
    \[
        \sigma\defeq\fun{rec}_{\fun2}(\univ,A,B)\colon\fun2\to\univ,
    \]
    with
    \[
        \sigma(0_{\fun2})\equiv A
        \quad\text{and}\quad
        \sigma(1_{\fun2})\equiv B.
    \]
    We can then introduce what we aim to prove as being equivalent to $A+B$,
    \[
        A \oplus B \defeq \sum_{x\colon\fun2} \sigma(x)
    \]
    along with
    \begin{equation}\label{eq:inA-inB}
        \begin{aligned}
            \fun{in}_A\colon A&\to A\oplus B, &a&\mapsto(0_{\fun2},a)\\
            \fun{in}_B\colon B&\to A\oplus B, &b&\mapsto(1_{\fun2},b).
        \end{aligned}
    \end{equation}
    To see that these are well-defined specifications, note that for any $a\colon A$, the pair $(0_{\fun2},a)$ has type $\sum_{x\colon\fun2}\sigma(x)$ because the second component is of the required type $\sigma(0_{\fun2})\equiv A$. A similar argument holds for $\fun{in}_B$ since $\sigma(1_{\fun2}) \equiv B$.

    It remains to be shown that $A+B\simeq A\oplus B$. To see this first define
    \begin{equation}\label{eq:phi-coproduct-to-sigma}
        \phi\colon A+B\to A\oplus B
    \end{equation}
    as the only map that satisfies
    \[
        \phi(\fun{inl}(a))\equiv\fun{in}_A(a)
        \quad\text{and}\quad
        \phi(\fun{inr}(b))\equiv\fun{in}_B(b)
    \]
    for $a\colon A$ and $b\colon B$. Second, to define a map in the other direction, we have to use the recursor evaluated at the coproduct $A+B$
    \[
        \fun{rec}_{\sum_{x\colon\fun2} \sigma(x)}(A+B)\colon
            \Bigl(\prod_{x\colon\fun2}\sigma(x)
                \to A+B\Bigr)
                \to \Bigl(\sum_{x\colon\fun2}\sigma(x)\Bigr)
                \to A+B
    \]
    and apply the result to a well-chosen dependent map
    \[
        h\colon\prod_{x\colon\fun2}\sigma(x)\to A+B.
    \]
    Take $h$ to be the one defined by
    \begin{align*}
        h(0_{\fun2})&\defeq\fun{inl}\colon A\to A+B,\\
        h(1_{\fun2})&\defeq\fun{inr}\colon B\to A+B,
    \end{align*}
    which is valid because $\sigma(0_{\fun2})\equiv A$ and $\sigma(1_{\fun2})\equiv B$.

    Now we can define
    \begin{equation}\label{eq:psi-sigma-to-coproduct}
        \psi\defeq\fun{rec}_{\sum_{x\colon\fun2}\sigma(x)}(A+B,h).
    \end{equation}
    Finally, we have to check the following two equalities
    \begin{align*}
        \phi(\psi(p))&\eq{A\oplus B}p\quad(p\colon A\oplus B)\\
        \psi(\phi(s))&\eq{A+B}s\quad(s\colon A+B)
    \end{align*}
    To verify the first equation, by the definition of $A\oplus B$, it suffices to consider the cases $p\defeq(0_{\fun2},a)$ and $p\defeq(1_{\fun2},b)$, with $a\colon A$ and $b\colon B$, since these exhaust all the canonical pairs of $A\oplus B$.
    \begin{itemize}
        \item For $p\defeq(0_{\fun2},a)$, we have $\psi((0_{\fun2},a)) \equiv h(0_{\fun2})(a) \equiv \fun{inl}(a)$. Thus,
        \[ \phi(\psi(p)) \equiv \phi(\fun{inl}(a)) \equiv \fun{in}_A(a) \equiv (0_{\fun2}, a) \equiv p. \]
        \item For $p\defeq(1_{\fun2},b)$, we have $\psi((1_{\fun2},b)) \equiv h(1_{\fun2})(b) \equiv \fun{inr}(b)$. Thus,
        \[ \phi(\psi(p)) \equiv \phi(\fun{inr}(b)) \equiv \fun{in}_B(b) \equiv (1_{\fun2}, b) \equiv p. \]
    \end{itemize}
    Conversely, for $s\colon A+B$, we check $\psi(\phi(s))\eq{A+B}s$ by induction on the coproduct:
    \begin{itemize}
        \item For $s\defeq\fun{inl}(a)$, $\psi(\phi(s)) \equiv \psi(\fun{in}_A(a)) \equiv \psi((0_{\fun2}, a)) \equiv \fun{inl}(a) \equiv s$.
        \item For $s\defeq\fun{inr}(b)$, $\psi(\phi(s)) \equiv \psi(\fun{in}_B(b)) \equiv \psi((1_{\fun2}, b)) \equiv \fun{inr}(b) \equiv s$.
    \end{itemize}
    This completes the proof.
\end{lpar}

\section{The Type of Natural Numbers}

\begin{lpar}{Natural numbers.}
    The type $\N\colon\univ$ is introduced with two constructors: the element \textsl{zero} $0\colon\N$ and a \textsl{successor} operator $\fun{succ}\colon\N\to\N$.

    Predictably, we adopt the conventional notation $1\defeq\fun{succ}(0)$, $2\defeq\fun{succ}(1)$, etc.

    The recursor
    \begin{align*}
        \fun{rec}_\N\colon\prod_{C\colon\univ}C\to(\N\to C\to C)
            &\to\N\to C
    \end{align*}
    is specified by
    \begin{align*}
        \fun{rec}_\N(C,c_0,c_s,0)&\defeq c_0,\\
        \fun{rec}_\N(C,c_0,c_s,\fun{succ}(n))
            &\defeq c_s(n,\fun{rec}_\N(C,c_0,c_s,n)).
    \end{align*}
    We can represent this in a definitionally commutative diagram
    \begin{equation}\label{tik:natural-rec}
        \begin{tikzcd}[row sep=large, column sep=huge]
            \fun1
                    \arrow[r,"0"]
                    \arrow[d,"{(0,c_0)}"']
                &\N
                    \arrow[r,"\fun{succ}"]
                    \arrow[d,"{(\id,\fun{rec})}" description]
                &\N
                    \arrow[d,"{(\id,\fun{rec})}"]\\
            \fun1\times C
                    \arrow[r,"0\times c_0"']
                &\N\times C
                    \arrow[r,"{(\fun{succ}\circ\pi_1, c_s)}"']
                &\N\times C
        \end{tikzcd}
    \end{equation}
    where $\fun{rec}$ stands for $\fun{rec}_\N(C,c_0,c_s)$.
    
    Note that this definition is different from the ones we introduced so far: it allows for $\fun{rec}$ to appear in the \rhs. This is why we say that $\N$ is an \textsl{inductive} type. 
    
    The justification of this extension is \textit{metatheoretic}: the expression on the \rhs\ uses $n$, which is syntactically \textit{shorter} than $\fun{succ}(n)$. This implies that well-formed terms effectively halt, after a finite number of iterations. More precisely, the syntactic reduction is \textsl{well-founded}, meaning that it has no infinite descending chains $x_1>x_2>\cdots>x_n>\cdots$.
    
    Consequently, to define a function $f\colon\N\to C$, we provide two pieces of information:
    \begin{equation}\label{eq:primitive-recursion}
        \begin{aligned}
            \text{\small a starting point } c_0&\colon C,\\
            \text{\small the next step function } c_s&\colon\N\to C\to C
        \end{aligned}
    \end{equation}
    and declare
    \begin{equation}\label{eq:recursion-semantics}
        \begin{aligned}
            f(0) &\defeq c_0,\\
            f(\fun{succ}(n))
                &\defeq c_s(n,f(n)),
        \end{aligned}
    \end{equation}
    where the second equation corresponds to
    \[
        c_s(n,f(n))\equiv 
            c_s(n,\underbrace{\fun{rec}_\N(C,c_0,c_s,n)}_{f(n)}).
    \]
    We say that $f$ is defined by \textsl{primitive recursion}.
\end{lpar}

\begin{lpar}{Natural doubling.}
    A simple example of a function is $\fun{double}\colon\N\to\N$, which can be defined by
    \begin{align*}
        \fun{double}(0)&\defeq 0,\\
        \fun{double}(\fun{succ}(n))&\defeq\fun{succ}(\fun{succ}(\fun{double}(n))),
    \end{align*}
    which corresponds to $c_0\defeq0$ and $c_s\colon\N\to\N\to\N$, $c_s(n,d)\defeq\fun{succ}(\fun{succ}(d))$. Thus, for instance,
    \begin{align*}
        \fun{double}(1) &\equiv \fun{double}(\fun{succ}(0))\\
            &\equiv \fun{succ}(\fun{succ}(\fun{double}(0)))\\
            &\equiv\fun{succ}(\fun{succ}(0))\\
            &\equiv \fun{succ}(1)\\
            &\equiv2.
    \intertext{and}
        \fun{double}(2) &\equiv \fun{double}(\fun{succ}(1))\\
            &\equiv \fun{succ}(\fun{succ}(\fun{double}(1)))\\
            &\equiv\fun{succ}(\fun{succ}(2))\\
            &\equiv \fun{succ}(3)\\
            &\equiv4.
    \end{align*}
    Note that even though the expression grows in length in some of the intermediate steps, it eventually succeeds in removing any circular reference to $\fun{rec}$, which is the precise meaning of ``halting''.
\end{lpar}

\begin{lpar}{Addition of natural numbers.}\label{lpar:natural-addition}
    The natural recursor allows the definition of multi-variable functions, such as addition, by letting the return type $C$ be a function type itself. We define
    \[
        \fun{add}\colon\N\to\N\to\N
    \]
    using primitive recursion with $C \defeq \N \to \N$, and specifying after \eqref{eq:primitive-recursion}
    \begin{align*}
        c_0\colon\N&\to\N,&c_0
            &\defeq\lambda n.\,n\\
        c_s\colon\N&\to(\N\to\N)\to(\N\to\N),
            &c_s&\defeq\lambda m.\,\lambda g.\,
                \lambda n.\,\fun{succ}(g(n)),
    \end{align*}
    where $g\colon\N\to\N$ acts as a placeholder for the recursive result. When the recursor computes, $g$ will be instantiated with the function $n\mapsto m+n$.
    
    We define $\fun{add}\defeq\fun{rec}_\N(\N\to\N,c_0,c_s)$. By the computation rules of the recursor, this definition satisfies the standard equations:
    \begin{align*}
        \fun{add}(0, n) &\equiv c_0(n) \equiv n,\\
        \fun{add}(\fun{succ}(m),n)&\equiv c_s(m,\fun{add}(m))(n)\\
        &\equiv \fun{succ}(\fun{add}(m, n)),
    \end{align*}
    which specify $\fun{add}$ by recursion on the first variable.
\end{lpar}

\begin{lpar}{Multiplication of natural numbers.}\label{lpar:natural-multiplication}
    To define
    \[
        \fun{mult}\colon\N\to\N\to\N
    \]
    we apply the recursion principle for $\N$ with $C\defeq\N\to\N$ and
    \begin{align*}
        c_0 &\defeq \lambda n.\, 0,\\
        c_s &\defeq \lambda m.\, \lambda g.\, \lambda n.\, \fun{add}(g(n), n).
    \end{align*}
    As a result, $\fun{mult}\defeq\fun{rec}_\N(C,c_0,c_s)$ satisfies the following equations:
    \begin{align*}
        \fun{mult}(0,n) &\equiv c_0(n) \equiv 0,\\
        \fun{mult}(\fun{succ}(m),n) &\equiv c_s(m,\fun{mult}(m))(n)\\
        &\equiv \fun{add}(\fun{mult}(m, n),n).
    \end{align*}    
\end{lpar}

\begin{lpar}{Pattern matching.}
    When we introduce a function $f\colon\N\to C$ such as
    \begin{align*}
        f(0) &\defeq \Phi_0,\\
        f(\fun{succ}(n)) &\defeq \Phi_s,
    \end{align*}
    where $\Phi_0$ is an expression of type $C$, and $\Phi_s$ is an expression of type $C$ which may involve the variable $n$ and also the symbol ``$f(n)$'', we may translate it to a formal definition
    \[
        f\colon\equiv\fun{rec}_\N(C,\Phi_0,\lambda n.\,
            \lambda g.\,\Phi'_s),
    \]
    where $\Phi'_s$ is obtained from $\Phi_s$ by replacing all occurrences of ``$f(n)$'' by the new variable $g$.
    
    This style of defining functions by recursion (or, more generally, dependent functions by induction) is called definition by \textsl{pattern matching}. We frequently use it for the sake of convenience.

    \needspace{2\baselineskip}
    \begin{xmpl}
        Let us define the factorial function $\fun{fact} : \mathbb{N} \to \mathbb{N}$.

        In the convenient pattern matching style, we write:
        \begin{align*}
            \fun{fact}(0) &\defeq 1, \\
            \fun{fact}(\fun{succ}(n)) &\defeq \fun{succ}(n) \cdot \fun{fact}(n),
        \end{align*}
        where the usual `$\cdot$' symbol denotes multiplication.
        
        Here, our components correspond to the general schema as follows:
        \begin{enumerate}[a), font=\upshape]
            \item The target type is $C \equiv \mathbb{N}$.
            \item The base case is $\Phi_0 \equiv 1$.
            \item The successor expression is $\Phi_s \equiv \fun{succ}(n) \cdot \fun{fact}(n)$. Note that this involves both the variable $n$ and the recursive call $\fun{fact}(n)$.
            \item $\Phi'_s \defeq \fun{succ}(n) \cdot g$.
            \item $\fun{fact}\equiv\fun{rec}_\N(\N,1,\lambda n.\,\lambda g.\,\fun{succ}\cdot g)$.
        \end{enumerate}
    \end{xmpl}
\end{lpar}

\begin{lpar}{Induction principle.}\label{lpar:induction-principle}
    Assume as given a family $C\colon\N\to\univ$, an element $c_0\colon C(0)$, and a function $c_s\colon\prod_{n\colon\N} C(n) \to C(\fun{succ}(n))$; then we can construct a dependent function $f\colon\prod_{n\colon\N} C(n)$ with the defining equations:
    \begin{align*}
        f(0) &\defeq c_0,\\
        f(\fun{succ}(n)) &\defeq c_s(n, f(n)).
    \end{align*}
    We formalize this with the induction function
    \[
        \fun{ind}_\N\colon\prod_{C\colon\N\to\univ}
            C(0) \to \Bigl(\prod_{n\colon\N}C(n) \to C(\fun{succ}(n))\Bigr)
            \to \prod_{n\colon\N}C(n)
    \]
    plus the equations
    \begin{align*}
        \fun{ind}_\N(C, c_0, c_s, 0) &\defeq c_0,\\
        \fun{ind}_\N(C, c_0, c_s, \fun{succ}(n)) &\defeq c_s(n, \fun{ind}_\N(C, c_0, c_s, n)).
    \end{align*}
    As usual, we will refer to $C$ as the \textsl{theorem}, to $c_0$ as the \textsl{base case}, and to $c_s$ as the \textsl{inductive step}. More precisely,
    \[
        c_s\defeq\lambda n.\,\lambda h.\, \textit{witness},
    \]
    where $n$ is a generic natural, $h\colon C(n)$ is our \textsl{inductive hypothesis} and \textit{witness} is an element of $C(\fun{succ}(n))$.\footnote{In other words, $c_s$ is a function that produces a proof of $C(\fun{succ}(n))$ from a proof of $C(n)$.}
\end{lpar}

\begin{lpar}{Blocked computation.}\label{lpar:blocked-computation}
    As usual, we will denote $\fun{add}(m,n)$ by $m+n$ and $\fun{mult}(m,n)$ by $m\cdot n$. In the first case, computation on the first argument implies the definitional equalities:
    \begin{equation}\label{eq:0+n-equiv-n}
        \begin{aligned}
            0+n &\equiv n,\\
            \fun{succ}(m)+n &\equiv \fun{succ}(m+n).
        \end{aligned}
    \end{equation}
    In particular, we can derive:
    \begin{align*}
        1+n &\equiv \fun{succ}(0)+n\\
            &\equiv \fun{succ}(0+n)\\
            &\equiv \fun{succ}(n).
    \end{align*}
    However, the equation $n \eq\N n+0$ does not hold definitionally for an arbitrary variable $n$. It is a proposition that must be proven as a theorem.

    The reason is found in the operational semantics: a \textit{machine} evaluating the expression $\Phi+0$ proceeds by pattern matching on $\Phi$ (the first argument). It checks if the \textit{head}\footnote{The word 'head' refers to the root of the Abstract Syntactic Tree (AST) of the expression.} of $\Phi$ matches the constructor $0$ or matches the constructor $\fun{succ}$.

    In our case, $\Phi\defeq n$. Since $n$ is a variable and not a constructor, neither reduction rule applies. The machine cannot simplify the expression further, and the computation \textit{blocks}. Thus, we cannot rely on automatic reduction. In \nref{lpar:additive-identity} we construct an explicit proof term.
\end{lpar}

\section{Propositions as Types}

\begin{lpar}{Propositional Logic.}\label{lpar:propositional-logic}
    There is an interpretation that assigns constructive logic semantics to type-theoretical types, which can be summarized as:
    \[
        \begin{tabular}{l|ll}
            \hline
            &\rule{0mm}{4mm}\textbf{\small Constructive proposition}
                &\textbf{\small Type Theory's type}\\
            \hline
            \rule{0mm}{4mm}1&true & $\mathbf{1}$ \\
            2&false & $\mathbf{0}$ \\
            3&$A$ and $B$ & $A \times B$ \\
            4&$A$ or $B$ & $A + B$ \\
            5&if $A$ then $B$ & $A \to B$ \\
            6&$A$ if and only if $B$ & $(A\to B)\times(B\to A)$\\
            7&not $A$ & $A \to \mathbf{0}$ \\
            \hline
        \end{tabular}
    \]
    Let us review them one by one, assuming that types $A$ and $B$ represent propositions and their elements represent proofs.
    \begin{enumerate}[$1.$]
        \item Since $\fun\ast\colon\fun1$, we see that $\fun1$ has a proof (it is always inhabited).

        \item Since $\fun0$ has no elements, there is no proof for $\fun0$ (it is false).

        \item A proof $x$ of $A\times B$ produces a proof $\pi_1(x)$ of $A$ and a proof $\pi_2(x)$ of $B$. Conversely, if $a$ is a proof of $A$ and $b$ is a proof of $B$, then the pair $(a,b)$ is a proof of $A\times B$.

        \item A proof of $A+B$ (propositionally) equals either $\fun{inl}(a)$, for a proof $a$ of $A$, or $\fun{inr}(b)$, for a proof $b$ of $B$. The converse clearly holds.

        \item A proof of the implication $A \to B$ is a function $f$. This function provides a method to transform any proof $a$ of $A$ into a proof $f(a)$ of $B$.

        \item This follows immediately from parts 3 and 5.

        \item By definition, a proof of $\neg A$ is a function $f\colon A\to\fun0$. If such a function exists, $A$ cannot have any proof, otherwise applying $f$ to that proof would yield a proof of $\fun0$ (which is impossible, see part 2).
    \end{enumerate}
\end{lpar}

\begin{lpar}{From English to types.}
    Using the table from \nref{lpar:propositional-logic} we can translate a classical proof into Type Theory. Consider De Morgan's law:
    \[
        \textit{\small if not $A$ and not $B$, then not $(A$ or $B)$.}
    \]
    This corresponds to the type
    \[
        (A\to\fun0)\times(B\to\fun0)\to (A+B\to\fun0).
    \]
    To construct an inhabitant $f$ of this type, we assume a witness for the antecedent, which is a pair $(x,y)$ where $x\colon A\to\fun0$ and $y\colon B\to\fun0$. 

    We must construct a return value of type $A+B \to \fun0$. We do this using the recursion principle for coproducts with the target type $C\defeq\fun0$.
    \needspace{2\baselineskip}
    The recursor requires two functions to handle the cases:
    \begin{itemize}
        \item A function $A \to \fun0$. Our assumption $x$ is exactly this function.
        \item A function $B \to \fun0$. Our assumption $y$ is exactly this function.
    \end{itemize}
    Therefore, we can define $f$ as (see \nref{lpar:coproduct-recursion})
    \[
        f((x,y))\defeq \fun{rec}_{A+B}(\fun0,\, x,\, y).
    \]
    Thus, for $a\colon A$ and $b\colon B$, we have 
    \[
        f((x,y))(\fun{inl}(a))\defeq x(a)
        \quad\text{and}\quad
        f((x,y))(\fun{inr}(b))\defeq y(b),
    \]
    which is well defined because $x(a),y(b)\colon\fun0$. Therefore, $f$ proves $(A+B)\to\fun0$ in each of the two possible cases derived from $A+B$.

    In contrast, the other law
    \[
        \textit{\small if not\/ $(A$ and\/ $B)$,
            then\/ $($not\/ $A)$ or\/ $($not\/ $B)$},
    \]
    which translates into
    \[
        ((A\times B)\to\fun0)\to(A\to\fun0)+(B\to\fun0)
    \]
    is not generally valid because there is no method to decide which of the two propositions, $A$ or $B$, is false from a proof of $\neg(A\times B)$. However, to exhibit an element of $(A\to\fun0)+(B\to\fun0)$, we must decide whether it will be a value of $\fun{inl}$ or of $\fun{inr}$. For instance, consider the case where $B\defeq\neg A$. We do know that $A\wedge\neg A$ is false\footnote{A witness of this proposition is a pair $(a,b)$ with $a\colon A$ and $b\colon A\to\fun0$. It cannot exist since otherwise $b(a)\colon\fun0$.} but not if this is because $A$ is false or $\neg A$ is false (as it happens with unsolved conjectures).
\end{lpar}

\begin{lpar}{Predicate logic.}\label{lpar:predicate-logic}
    The following table extends the propositions as types interpretation (see \nref{lpar:propositional-logic}) to quantifiers
    \[
        \begin{tabular}{l|ll}
            \hline
            &\rule{0mm}{4mm}\textbf{\small Predicate logic}
                &\textbf{\small Type Theory's type}\\[0.3mm]
            \hline
            \rule{0mm}{4mm}1&for all $x\colon A$, $P(x)$ holds
                &$\prod_{x\colon A}P(x)$ \\[1mm]
            2&there exists $x\colon A$ such that $P(x)$
                &$\sum_{x\colon A}P(x)$\\[1mm]
            \hline
        \end{tabular}
    \]
    For example, the logical distributivity law
    \[
        \textit{\small if for all\/ $x\colon A$, $P(x)$ and\/ $Q(x)$,
            then $($for all\/ $x\colon A$, $P(x))$ and\/ $($for all\/ $x\colon A$, $Q(x))$}
    \]
    translates into the type
    \[
        \Bigl(\prod_{x\colon A}(P(x)\times Q(x))\Bigr)
            \to\Bigl(\prod_{x\colon A}P(x)\Bigr)\times\Bigl(\prod_{x\colon A}Q(x)\Bigr).
    \]
    Its proof consists of constructing a witness $f$ for this type.

    Take a dependent function $r$ in the domain. We must construct a pair of dependent functions $(p,q)$, where $p\colon \prod_{x\colon A}P(x)$ and $q\colon \prod_{x\colon A}Q(x)$.
    For any $x\colon A$, the term $r(x)$ is a pair in $P(x)\times Q(x)$. Using standard projections, we could define the result directly as:
    \[
        f(r) \defeq \bigl(\lambda x.\,\pi_1(r(x)),\; \lambda x.\,\pi_2(r(x))\bigr).
    \]
    To derive this using the recursor, we must separate the components by providing the appropriate step functions. For the first component $p$, the handler must select the first argument:
    \[
        p(r)\defeq\lambda x.\,\fun{rec}_{P(x)\times Q(x)}(P(x),\,\lambda u.\,\lambda v.\,u,\,r(x)).
    \]
    Similarly, for the second component $q$, the handler selects the second argument:
    \[
        q(r)\defeq\lambda x.\,\fun{rec}_{P(x)\times Q(x)}(Q(x),\,\lambda u.\,\lambda v.\,v,\,r(x)).
    \]
    Finally, we assemble $f$ as the pair of these lambda abstractions:
    \[
        f\defeq\lambda r.\,(p(r),q(r)).
    \]
\end{lpar}

\section{Identity Types}\label{sec:identity-types}

In this section we go in further detail about identity family types. Concretely, if $A\colon\univ$, then $\Id_A\colon A\to A\to\univ$ is the type family dependent on two copies of~$A$, whose notation is 
\[
    \Id_A(a,b)\defeq a\eq Ab.
\]
The witnesses of this type are regarded as \textit{paths in the space $A$ from the point $a$ to the point $b$}. The constructor of elements of this type is the dependent \textsl{reflexivity} function
\[
    \fun{refl}\colon\prod_{a\colon A}a\eq Aa.
\]
In particular, if $b\equiv a$, then $\fun{refl}_a\colon a\eq Ab$.

\begin{lpar}{Path recursion.}
    Consider the type of \textsl{free} paths
    \[
        A^I \defeq \sum_{x,y\colon A} x\eq A y.
    \]
    While this represents the space of all paths in $A$, the fundamental recursion principle is stated for paths starting at a fixed element $x\colon A$.

    Given a type $B$, the recursion principle for the \textsl{based} path space at $x$ defines functions that effectively ignore the path. It has the signature:
    \[
        \fun{rec}_{\eq A}\colon\prod_{B\colon\univ}B
            \to\prod_{y\colon A}(x\eq Ay)\to B.
    \]
    It satisfies the computation rule:
    \[
        \fun{rec}_{\eq A}(B,c_{\fun{refl}},x,\fun{refl}_x)\equiv c_{\fun{refl}},
    \]
    where $c_{\fun{refl}}\colon B$ is the value assigned to the reflexivity path.

    Since $x$ is fixed in the context, the value $b$ may depend on it. For example, taking $B \defeq A$ and $b \defeq x$, we obtain the function
    \begin{align*}
        f &\colon \prod_{y\colon A} (x\eq A y) \to A \\
        f &\defeq \lambda y.\,\lambda p.\,x,
    \end{align*}
    which is formally defined as $f\defeq\fun{rec}_{\eq A}(A,x)$, and satisfies $f(x,p)\eq Ax$ for any path $p$ with origin in $x$.\footnote{Compare this with the function $g\defeq\lambda x.\,\lambda p.\,x$ which satisfies $g(x)\equiv x$ for $x\colon A$.}
\end{lpar}

\begin{lpar}{General path induction.}
    In the case of identity types, the induction principle (a.k.a.~\textsl{path induction}) conceptually requires a motive defined on the total space of paths:
    \[
        \Bigl(\sum_{x,y\colon A}x\eq A y\Bigr)\to\univ.
    \]
    In Type Theory, however, we specify this map using its Curried\footnote{See \nref{lpar:generalized-currying}.} form:
    \[
        C\colon\prod_{x,y\colon A}(x\eq A y)\to\univ.
    \]
    This distinction explains why we write $C(x,y,p)$ rather than $C((x,y,p))$.

    Given such a motive $C$ and a base case function $c\colon\prod_{x\colon A}C(x,x,\fun{refl}_x)$, the principle establishes the existence of a dependent function
    \[
        f\colon\prod_{x,y\colon A}\;\prod_{p\colon x\eq Ay}C(x,y,p)
    \]
    such that $f(x,x,\fun{refl}_x)\defeq c(x)$ for $x\colon A$. In other words, the induction principle for identity types is defined as
    \begin{equation}\label{eq:ind=A}
        \fun{ind}_{\eq A}\colon\prod_{C\colon\prod_{x,y\colon A}(x\eq A y)\to\univ}
            \;\biggl(\prod_{x\colon A}C(x,x,\fun{refl}_x)\biggr)
                \to\prod_{x,y\colon A}\;\prod_{p\colon x\eq Ay}C(x,y,p)
    \end{equation}
    satisfying the computation rule
    \[
        \fun{ind}_{\eq A}(C,c,a,a,\fun{refl}_a)\defeq c(a)
    \]
    for every $a\colon A$.
\end{lpar}

\begin{lpar}{Path symmetry.}
    The following lemma formalizes the symmetry of the \textsl{equality relation} between elements of the same type $A$
    \begin{equation}\label{eq:path-relation}
        x\sim_A y\iff x\eq Ay \textsl{ is inhabited.}
    \end{equation}
    
    \begin{lem}
        For every type\/ $A$ and every\/ $x,y\colon A$, there is a function
        \[
            (x\eq A y) \to (y\eq A x),
        \]
        denoted\/ $p\mapsto p^{-1}$, such that\/ $\fun{refl}_x^{-1}\equiv\fun{refl}_x$ for each\/ $x\colon A$.
    \end{lem}
    
    \begin{proof}
        Given a type $A$, define the \textsl{motive} family\footnote{The type family whose role is to identify the theorem we are about to prove.}
        \begin{align*}
            C&\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ\\
            C(x,y,p)&\defeq(y\eq Ax).
        \end{align*}
        For the base case, we need a function of type $\prod_{x\colon A}C(x,x,\fun{refl}_x)$. Since
        \[
            C(x,x,\fun{refl}_x)\equiv(x\eq Ax),
        \]
        we can take
        \[
            c\defeq\lambda x.\,\fun{refl}_x.
        \]
        Then, by general path induction, we define
        \[
            p^{-1} \defeq \fun{ind}_{\eq A}(C,c,x,y,p)
        \]
        with
        \[
            \fun{refl}_x^{-1}
                \equiv\fun{ind}_{\eq A}(C,c,x,x,\fun{refl}_x)
                \equiv c(x)
                \equiv\fun{refl}_x,
        \]
        which completes the proof.
    \end{proof}
\end{lpar}

\begin{lpar}{Path concatenation.}\label{lpar:path-concatenation}
    The following lemma shows that the equality relation is transitive.
    \begin{lem}\label{lem:path-concatenation}
        For every type\/ $A$ and every\/ $x,y,z\colon A$ there is a \textsl{concatenation} function
        \[
            (x\eq Ay)\to(y\eq Az)\to(x\eq Az),
        \]
        written
        \[
            p\mapsto q\mapsto p\ct q,
        \]
        such that\/ $\mathsf{refl}_x\ct q\equiv q$ for any\/ $x\colon A$ and $q\colon x\eq Az$.
    \end{lem}

    \begin{proof}
        Let $D\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ$ be the motive family 
        \[
            D(x,y,p) \defeq \prod_{z\colon A}\;\prod_{q\colon y\eq Az}x\eq Az.
        \]
        Since $x\eq Az$ does not depend on the path $q\colon y\eq Az$, by \eqref{eq:function=independent-dependent-function} we can rewrite $D$ as
        \[
            D(x,y,p) \defeq \prod_{z\colon A}(y\eq Az)\to(x\eq Az).
        \]
        For the base case we take
        \[
            d(x)\colon D(x,x,\fun{refl}_x)
                \equiv\prod_{z\colon A}\;(x\eq Az)\to (x\eq Az),\quad d(x)\defeq\lambda z.\,\lambda q.\,q.
        \]
        By path induction, we obtain
        \[
            \fun{ct}(x,y,p)\defeq\fun{ind}_{\eq A}(D,d,x,y,p)\colon D(x,y,p).
        \]
        We can define the concatenation $p\ct q$ by applying this function:
        \[
            p\ct q\defeq \fun{ct}(x,y,p,z,q).
        \]
        The computation rule for path induction implies
        \[
            \fun{ct}(x,x,\fun{refl}_x)\equiv d(x).
        \]
        Therefore,
        \[
            \fun{refl}_x\ct q\equiv \fun{ct}(x,x,\fun{refl}_x,z,q)
                \equiv d(x)(z,q)\equiv q,
        \]
        which completes the proof.
    \end{proof}
\end{lpar}

\begin{lpar}{Right unit law.}\label{lpar:right-unit-law}
    The term $\fun{refl}$ also acts as a right unit for concatenation; however, in this case, the equality is propositional.
    
    \begin{lem}\label{lem:right-unit-law}
        For every type\/ $A$, every\/ $x,y\colon A$ and every path\/ $p\colon x\eq Ay$, we have a path
        \[
            \fun{unit}_r(p)\colon p\ct\fun{refl}_y\eq{x\eq Ay}p.
        \]
    \end{lem}
    
    \begin{proof}
        We perform path induction on $p$. Let $C$ be the motive family
        \begin{align*}
            C&\defeq\prod_{x,y\colon A}\;\prod_{p\colon x\eq Ay}
                (p\ct\fun{refl}_y\eq{x\eq Ay}p).
        \end{align*}
        For the base case, we assume $y \equiv x$ and $p \equiv \fun{refl}_x$. We must construct a term of type
        \[
            \fun{refl}_x\ct\fun{refl}_x\eq{x\eq Ax}\fun{refl}_x.
        \]
        By Lemma~\ref{lem:path-concatenation}, we have the definitional equality $\fun{refl}_x\ct\fun{refl}_x\equiv\fun{refl}_x$. Thus, the goal simplifies to $\fun{refl}_x\eq{x\eq Ax}\fun{refl}_x$, which is inhabited by $c(x)\defeq\fun{refl}_{\fun{refl}_x}$.
    
        Applying path induction, we obtain the desired term
        \[
            \fun{unit}_r(p) \defeq \fun{ind}_{\eq A}(C,c,x,y,p).
        \]
    \end{proof}
\end{lpar}

\begin{lpar}{Further concatenation properties.}

\begin{lem}\label{lem:concat}
    Let\/ $A\colon\univ$,\/ $x,y,z, w\colon A$ and\/ $p\colon x\eq Ay$. Then, the following types are inhabited:
    \begin{enumerate}[a), font=\upshape]
        \item $p^{-1}\ct p\eq{y\eq Ay}\fun{refl}_y$ and\/ $p\ct p^{-1}\eq{x\eq Ax}\fun{refl}_x$.
        \item $(p^{-1})^{-1}\eq{x\eq Ay}p$.
        \item $p\ct(q\ct r)\eq{x\eq Aw}(p\ct q)\ct r$, for\/ $q\colon y\eq Az$ and\/ $r\colon z\eq Aw$.
    \end{enumerate}
\end{lem}

\begin{proof}${}$
    \begin{enumerate}[a), font=\upshape]
        \item Consider the motive
        \[
            C\defeq\prod_{x,y\colon A}\;\prod_{p\colon x\eq Ay}
                p^{-1}\ct p\eq{y\eq Ay}\fun{refl}_y.
        \]
        We must show $C(x,x,\fun{refl}_x)$. Since $\fun{refl}_x^{-1}\equiv\fun{refl}_x$ and $\fun{refl}_x\ct\fun{refl}_x\equiv\fun{refl}_x$, we have
        \[
             \fun{refl}_x^{-1}\ct\fun{refl}_x \equiv \fun{refl}_x.
        \]
        Thus, we can take $c(x)\defeq\fun{refl}_{\fun{refl}_x}$ and obtain
        \[
            \fun{ind}_{\eq A}(C,c,x,y,p)\colon C(x,y,p).
        \]
        The other equation is proved similarly.\footnote{Alternatively, it can be derived from part~b) and the equation just proved, applied to $p^{-1}$ in the place of~$p$.}

        \item By path induction on $p$, it suffices to consider the case where $y\equiv x$ and $p\equiv\fun{refl}_x$. In that case, the statement holds definitionally because $\fun{refl}_x^{-1}\equiv\fun{refl}_x$.

        \item By repeated path induction on $p$, $q$, and $r$, it suffices to consider the case where they are all $\fun{refl}_x$. In that case, the statement holds because $\fun{refl}_x\ct\fun{refl}_x\equiv\fun{refl}_x$. \qedhere
    \end{enumerate}
\end{proof}
    
\end{lpar}

\begin{lpar}{Transport.}\label{lpar:transport}
    The principle of \textsl{indiscernibility of identicals} establishes that the equality constructor is compatible with \textsl{type transport} (a.k.a.~\textsl{type substitution}).\footnote{See also Exercise~\ref{exr:ap-application}.}
        
\begin{lem}
    Let\/ $P\colon A\to\univ$ be a family of types. Given\/ $x,y\colon A$ and a path\/ $p\colon x\eq A y$, there is a function
    \[
        \fun{transport}^P(p)\colon P(x)\to P(y).
    \]
    Moreover, $\fun{transport}^P(\fun{refl}_x) \equiv \lambda z.\,z$.
\end{lem}

\begin{proof}
    Consider the motive $C\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ$
    \[
        C(x,y,e)\defeq P(x)\to P(y).
    \]
    For the base case $c\colon C(x,x,\fun{refl}_x)\equiv P(x)\to P(x)$ take the identity function $c\defeq\lambda z.\,z$. Then define
    \[
        \fun{transport}^P(x,y,e)\defeq\fun{ind}_{\eq A}(C,c,x,y,e).
    \]
\end{proof}    

\end{lpar}

\begin{lpar}{Based path induction.}\label{lpar:based-path-induction}
    A different approach to path induction, which we will show is equivalent to the general one, consists in fixing an element $a\colon A$ before considering the type family~$D$, specific for $a$,
    \[
        D\colon\prod_{x\colon A}(a\eq Ax)\to\univ
    \]
    and an element $d\colon D(a,\fun{refl}_a)$ and obtain a dependent function
    \[
        f\colon\prod_{x\colon A}\;\prod_{p\colon a\eq Ax}D(x,p).
    \]
    This means that to define an element of the family $D$ for all $x$ and $p$, it suffices to consider the case where $x$ is $a$ and $p$ is $\fun{refl}_a$.
    
    As a function, based path induction becomes
    \begin{equation}\label{eq:ind'=A}
    \fun{ind}'_{\eq A}\colon\prod_{a\colon A}
        \;\prod_{D\colon\prod_{x\colon A}(a\eq Ax)\to\univ}
        D(a,\fun{refl}_a)
        \to
        \prod_{x\colon A}\;\prod_{p\colon a\eq Ax}D(x,p)
    \end{equation}
    with the equality
    \[
        \fun{ind}'_{\eq A}(a,D,d,a,\fun{refl}_a)\defeq d.
    \]
    \textbf{Note.} The difference between both forms of path induction is that in the based one we are free to choose and fix an element $a\colon A$ and then (1)~formulate a property $D(x,p)$ for every $p\colon a\eq Ax$, and (2)~exhibit a witness of $D(a,\fun{refl}_a)$. In general path induction, we formulate $C(x,y,p)$ for generic $p\colon x\eq Ay$, and then prove $C(x,x,\fun{refl}_x)$, also for a generic $x\colon A$. While the general approach may look natural in some special cases, the based one is often simpler because the generalization $D(x,p)$ it requires is closer to the concrete base case $D(a,\fun{refl}_a)$, with $a$ fixed, than the full generalization $C(x,y,p)$ required by the $\fun{ind}$ function. As we will see next, both notions are equivalent.
\end{lpar}

\begin{lpar}{Path induction equivalence.}
    Here we will see that the two path induction principles are equivalent. First, observe that general path induction $\fun{ind}_{\eq A}$ can be obtained from based path induction $\fun{ind}'_{\eq A}$ by treating the starting point $x$ as the parameter for the based version. We define:
    \[
        \fun{ind}_{\eq A}(C,c,x,y,p)
        \defeq
            \fun{ind}'_{\eq A}(x,\lambda y.\,\lambda p'.\,C(x,y,p'),
            c(x),y,p).
    \]
    To see that this is well defined, consider the types required by $\fun{ind}_{\eq A}$:
    \begin{align*}
        C &\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ\\
        c &\colon \prod_{x\colon A}C(x,x,\fun{refl}_x)\\
        x,y &\colon A\\
        p &\colon x\eq Ay
    \end{align*}
    By fixing the first argument to $x$, we obtain the specific arguments required by $\fun{ind}'_{\eq A}$ with base point $x$:
    \begin{align*}
        D&\defeq\lambda y.\,\lambda p'.\,C(x,y,p')\colon\prod_{y\colon A}(x\eq Ay)\to\univ\\
        d &\defeq c(x) \colon C(x,x,\fun{refl}_x)\\
        y &\colon A\\
        p &\colon x\eq A y,
    \end{align*}
    which honors the signature of $\fun{ind}'_{\eq A}$. Moreover,
    \[
        \fun{ind}_{\eq A}(C,c,x,x,\fun{refl}_x)
        \equiv \fun{ind}'_{\eq A}(x,D,d,x,\fun{refl}_x)
        \equiv d\equiv c(x),
    \]

    For the converse, fix  $a\colon A$ and suppose we are given
    \[
        D\colon\prod_{x\colon A}(a\eq Ax)\to\univ, \quad \text{and} \quad d\colon D(a,\fun{refl}_a).
    \]
    In order to derive  $D$ using  $\fun{ind}$ we define the motive  $C$ as
    \begin{equation}\label{eq:ind-domain-function}
        C(u,v,q\colon u\eq Av)\defeq\prod_{s\colon a\eq Au}D(u,s)\to D(v,s\ct q).
    \end{equation}
    For the base case  $c(u)$ we need a term of type
    \[
        C(u,u,\fun{refl}_u)\equiv\prod_{s\colon a\eq Au}D(u,s)\to D(u,s\ct\fun{refl}_u).
    \]
    By \nref{lpar:right-unit-law}, there is a path  $\fun{unit}_r(s)\colon s\ct\fun{refl}_u\eq{a\eq Au}s$. Then, its inverse path satisfies  $\fun{unit}_r(s)^{-1}\colon s\eq{a\eq Au}s\ct\fun{refl}_u$.
    
    Since we need to transform terms of $D(u,s)$ into terms of $D(u,s\ct\fun{refl}_u)$, we define $P_u\defeq\lambda s.\,D(u,s)$ and use the function $\fun{transport}^{P_u}$
    \begin{equation}\label{eq:ind-base-case-subst}
        c(u)\defeq\lambda s.\,\fun{transport}^{P_u}(\fun{unit}_r(s)^{-1})
            \colon C(u,u,\fun{refl}_u).
    \end{equation}
    By path induction, we obtain
    \[
        E(u,v,q) \defeq\fun{ind}_{\eq A}(C,c,u,v,q)\colon C(u,v,q),
    \]
    which yields a witness for the based induction:
    \[
        f(x,p)\defeq E(a,x,p)(\fun{refl}_a)(d).
    \]
    This is well-typed because, according to \eqref{eq:ind-domain-function},  $E(a,x,p)(\fun{refl}_a)$ defines a map  $D(a,\fun{refl}_a)\to D(x,\fun{refl}_a\ct p)$, and since  $\fun{refl}_a\ct p\equiv p$, the codomain is precisely  $D(x,p)$. Finally, we apply this map to $d\colon D(a,\fun{refl}_a)$ to get an element of $D(x,p)$.
    
    To verify the computation rule, we have to prove that
    \[
        f(a,\fun{refl}_a)\equiv d.
    \]
    Expanding the definition,
    \begin{align*}
        f(a,\fun{refl}_a)
            &\equiv\fun{ind}_{\eq A}(C,c,a,a,\fun{refl}_a)(\fun{refl}_a)(d)\\
            &\equiv c(a)(\fun{refl}_a)(d).
    \end{align*}
    We reduce the term  $c(a)(\fun{refl}_a)$:
    \begin{align*}
        c(a)(\fun{refl}_a)
            &\equiv\fun{transport}^{P_a}(\fun{unit}_r(\fun{refl}_a)^{-1})
                &&;\ \eqref{eq:ind-base-case-subst}\\
            &\equiv\fun{transport}^{P_a}(\fun{refl}_a^{-1})
                &&;\ \fun{unit}_r(\fun{refl}_a)\equiv\fun{refl}_a\\
            &\equiv\fun{transport}^{P_a}(\fun{refl}_a)
                &&;\ \fun{refl}_a^{-1}\equiv\fun{refl}_a\\
            &\equiv\lambda z.\,z.
    \end{align*}
    Hence,
    \[
        f(a,\fun{refl}_a)\equiv (\lambda z.\,z)(d) \equiv d,
    \]
    as desired.
\end{lpar}

\section{Arithmetic of Natural Numbers}

\begin{lpar}{Additive identity.}\label{lpar:additive-identity}
    Since our definition of addition in $\N$ is by induction in the first operand, we know that $0+n\equiv n$ for $n\colon\N$. It remains to show the following
    
    \begin{lem}
        For every $n\colon\N$ the type $n+0\eq\N n$ is inhabited.
    \end{lem}

    \begin{proof} We proceed by induction on $n$.
    \begin{itemize}
        \item \textbf{Base case ($n\defeq0$):} We must show $0 \eq\N 0 + 0$. Since $0+0\equiv0$, this holds by reflexivity.
        \item \textbf{Inductive step:} We assume the hypothesis $h\colon n \eq\N n+0$ and must show $\fun{succ}(n) \eq\N \fun{succ}(n)+0$.
        By the computation rules, the \rhs\ reduces: $\fun{succ}(n)+0 \equiv \fun{succ}(n+0)$.
        Thus, the goal becomes $\fun{succ}(n) \eq\N \fun{succ}(n+0)$.

        To complete the proof, we need to apply the function $\fun{succ}$ to both sides of the hypothesis $h$. This relies on the \textit{principle of congruence} that states that functions respect equality.\footnote{Exercise~\ref{exr:ap-application}.}
        
        However, we can also use \nref{lpar:transport} as follows. Consider the type family $P\colon\N\to\univ$ defined by
        \[
            P\defeq\lambda k.\,\fun{succ}(n)\eq\N\fun{succ}(k).
        \]
        Since $\fun{refl}_{\fun{succ}(n)}\colon P(n)$, we can evaluate 
        \[
            \fun{transport}^P(h)\colon P(n)\to P(n+0)
        \]
        at $\fun{refl}_{\fun{succ}(n)}$ to construct an inhabitant of $\fun{succ}(n)\eq\N\fun{succ}(n+0)$, as desired. 
    \end{itemize}        
    \end{proof}
\end{lpar}

\begin{lpar}{Associativity of addition.}\label{lpar:associativity-of-addition}
    Proving the associativity of the addition of natural numbers entails defining a function
    \[
        \fun{assoc}\colon\prod_{i,j,k\colon\N}i+(j+k)\eq\N(i+j)+k.
    \]
    We proceed by induction on $i$. The type family $C\colon\N\to\univ$ is given by:
    \[
        C(i) \defeq \prod_{j,k\colon\N} i+(j+k)\eq\N(i+j)+k.
    \]
    To define the function $\fun{assoc} \defeq \fun{ind}_\N(C, c_0, c_s)$, we need to specify:
    \begin{align*}
        c_0 &\colon C(0),\\
        c_s &\colon \prod_{i\colon\N} C(i) \to C(\fun{succ}(i)).
    \end{align*}
    For the base case $c_0$, we need a witness of $0+(j+k)\eq\N(0+j)+k$. From the definition of addition we know that $0+n \equiv n$. Therefore, the equality reduces to $j+k\eq\N j+k$, and we can define the witness by reflexivity:
    \[
        c_0 \defeq \lambda j.\, \lambda k.\, \fun{refl}_{j+k}.
    \]
    
    For the inductive step $c_s$, we assume $i\colon\N$ and the inductive hypothesis $h\colon C(i)$. We must construct a proof for $\fun{succ}(i)$.
    \[
        c_s\defeq\lambda i.\,\lambda h.\,\lambda j.\,\lambda k.\,\dots
    \]
    where $h(j,k)\colon i+(j+k)\eq\N(i+j)+k$.
    
    Computing the goal, we obtain
    \begin{align*}
        \fun{succ}(i)+(j+k) &\equiv \fun{succ}(i+(j+k))\\
            &\eq\N \fun{succ}((i+j)+k) && \text{; by $h(j,k)$ + congruence}\\
            &\equiv \fun{succ}(i+j)+k\\
            &\equiv (\fun{succ}(i)+j)+k.
    \end{align*}
    The equality in the second line is justified by applying the function $\fun{succ}$ to the path provided by the hypothesis $h(j,k)$. This relies on the \textit{principle of congruence} ($\fun{ap}$), which we prove in Exercise~\ref{exr:ap-application}. Thus, the expression for $c_s$ is
    \[
        c_s\defeq\lambda i.\,\lambda h.\,\lambda j.\,\lambda k.\,
            \fun{ap}_{\fun{succ}}(h(j,k)).
    \]
\end{lpar}

\begin{lpar}{Successor injectivity.}
    The \textsl{predecessor} function $\fun{pred}\colon\N\to\N$ is inductively defined by
            \begin{align*}
                \fun{pred}(0)&\defeq0,\\
                \fun{pred}(\fun{succ}(n))&\defeq n.
            \end{align*}

    \begin{lem}\label{lem:succ-ne-0}
        For every\/ $n\colon\N$, $\fun{succ}(n)\neq0$.
    \end{lem}
    
    \begin{proof}
        We must construct a map of signature
        \[
            \prod_{n\colon\N}\fun{succ}(n)\eq\N0\to\fun0.
        \]
        Define a type family $P\colon\N\to\univ$ by recursion:
        \begin{align*}
            P(0)&\defeq\fun1\\
            P(\fun{succ}(n))&\defeq\fun0.
        \end{align*}
        Take $n\colon\N$ and suppose we have a path $p\colon\fun{succ}(n)\eq\N0$. By transport along the inverse path $p^{-1}\colon0\eq\N\fun{succ}(n)$, for $\fun\star\colon\fun1$, we obtain:
        \[
            \fun{transport}^P(p^{-1})\colon P(0)\to P(\fun{succ}(n)).
        \]
        The definition of $P$ implies that the desired map is $(n,p)\mapsto\fun{subst_P}(p^{-1})(\fun\star)$.
    \end{proof}

    \begin{lem}
        For\/ $n,m\colon\N$, if\/ $\fun{succ}(n)\eq\N\fun{succ}(m)$ is inhabited, then\/ $n\eq\N m$ is inhabited.
    \end{lem}
    
    \begin{proof}
        Fix $m\colon\N$. We have to define a function of type
        \[
            \prod_{n\colon\N}(\fun{succ}(n)\eq\N\fun{succ}(m)) \to (n\eq\N m).
        \]
        This is accomplished by applying the congruence principle $\fun{ap}_{\fun{pred}}$ to every given path, namely
        \[
            \lambda n.\,\lambda p.\,\fun{ap}_{\fun{pred}}(p).
        \]
    \end{proof}
    
\end{lpar}

\begin{lpar}{The order relation on $\N$.}
    The translation table (see \nref{lpar:predicate-logic}) implies that the relation $n\le m$, where $n,m\colon\N$, can be defined as
    \[
        n\le m \defeq \sum_{k\colon\N} (n+k\eq\N m).
    \]
    \begin{lem}
        The following properties hold
        \begin{enumerate}[a),font=\upshape]
            \item $0\le n$
            \item $n\le\fun{succ}(n)$
            \item $n\le n$
            \item $n\le m$ and $m\le n$ imply\/ $n=m$
            \item $n\le m$ and $m\le r$ imply\/ $n\le r$
        \end{enumerate}        
    \end{lem}
    \begin{proof} (Sketch)
        \begin{enumerate}[a)]
            \item Take $k \defeq n$. By the definition of addition (see \nref{lpar:natural-addition}), we have the definitional equality $0+n \equiv n$. Thus, $\fun{refl}_n$ has type $0+n \eq\N n$. The proof term is $(n,\fun{refl}_n)$.

            \item This relies on the equality $\fun{succ}(n)\eq\N n+1$, which can be proved by induction following a similar approach to the one we sketched in \nref{lpar:additive-identity} for proving $n+0\eq\N n$ .

            \item This is because $n+0\eq\N n$.

            \item The standard mathematical argument goes like this: Take $k$ and $h$ satisfying $n+k\eq\N m$ and $m+h\eq\N n$. Then $n+(k+h)\eq\N n$ and in order to derive $h\eq\N0$ and $k\eq\N0$, we need two theorems
            \begin{align*}
                n+s\eq\N n&\to s\eq\N0\quad\text{and}\\
                k+h\eq\N0&\to(k\eq\N0)\times(h\eq\N0).
            \end{align*}
            All of this can be easily expressed and proved formally.
            
            The first theorem follows by induction on $n$. The base case is trivial since $0+s\equiv s$. The inductive step consists in showing a witness of
            \[
                \fun{succ}(n)+s\eq\N\fun{succ(n)}
                    \to\fun{succ}(n+s)\eq\N\fun{succ}(n),
            \]
            which follows from $\fun{succ}(n)\eq\N\fun{succ}(m)\to n\eq\N m$.

            The second theorem follows from induction on $k$. The base case, with $k\equiv0$, is trivial. The inductive step is also trivial because
            \[
                \fun{succ}(k)+h\equiv\fun{succ}(k+h)
            \]
            and so, by Lemma~\ref{lem:succ-ne-0}, there is no inhabitant of $\fun{succ}(k)+h\eq\N0$.

            \item Take $k$ and $h$ so that $n+k\eq\N m$ and $m+h\eq\N r$. Then,
            \[
                (n+k+h\eq\N m+h)\times(m+h\eq\N r)
            \]
            is inhabited, and the result is a consequence of the transitivity of the equality.
        \end{enumerate}
    \end{proof}
\end{lpar}

\section{Exercises}

\begin{exr}\label{exr:function-composition}${}$
    \begin{enumerate}[a),font=\upshape]
        \item Given\/ $f \colon A \to B$ and\/ $g \colon B \to C$, define their composite\/ $g \circ f \colon A \to C$.
    
        \item Show that we have\/ $h \circ (g \circ f) \equiv (h \circ g) \circ f$.
    \end{enumerate}
\end{exr}

\begin{solution}${}$
    \begin{enumerate}[a)]
        \item We define the composition $g\circ f\colon A\to C$ as
        \[
            g\circ f \defeq\lambda x.\,g(f(x)).
        \]
        To check that this is valid we have to verify that $g(f(x))\colon C$, assuming $x\colon A$. But, given that $f\colon A\to B$, we know that $f(x)\colon B$. And since $g\colon B\to C$, we obtain $g(f(x))\colon C$.
        
        \item Based on part a), we define the polymorphic composition operator as
        \[
            \circ\colon \prod_{A,B,C\colon\univ}\;
                \prod_{g\colon B\to C}\;
                \prod_{f\colon A\to B}A\to C,
        \]
        with specification
        \[
            \circ \defeq
                \lambda A.\,\lambda B.\,\lambda C
                .\,\lambda g.\,\lambda f.\,\lambda x
                .\,g(f(x)).
        \]
        Therefore, assuming $h\colon C\to D$, $g\colon B\to C$ and $f\colon A\to B$, we obtain
        \[
            g\circ_{ABC}f\colon A\to C\quad\text{and}\quad
            h\circ_{BCD}g\colon B\to D.
        \]
        Therefore,
        \[
            h\circ_{ACD}(g\circ_{ABC}f)\colon A\to D\quad\text{and}\quad
            (h\circ_{BCD}g)\circ_{ABD}f\colon A\to D.
        \]
        Moreover,
        \begin{align*}
            h\circ_{ACD}(g\circ_{ABC}f)
                &\defeq \lambda x.\,h((g\circ f)(x))\\
                &\phantom\defeq
                    \lambda x.\,h((\lambda y.\,g(f(y)))(x))\\
                &\phantom\defeq\lambda x.\,h(g(f(x))
                    &&\text{; by part a)}\\
        \intertext{and}
            (h\circ_{BCD}g)\circ_{ABD}f
                &\defeq\lambda x.\,(h\circ g)(f(x))\\
                &\phantom\defeq
                    \lambda x.\,(\lambda y.\,h(g(y)))(f(x))\\
                &\phantom\defeq\lambda x.\,h(g(f(x))
                    &&\text{; $h\circ g \defeq\lambda y.\,h(g(y))$}
        \end{align*}
        show that $h\circ_{ACD}(g\circ_{ABC}f)\equiv(h\circ_{BCD}g)\circ_{ABD}f$.
    \end{enumerate}
\end{solution}

\begin{exr}
    Derive the recursion principle for products\/ $\fun{rec}_{A\times B}$ using only the projections, and verify that the definitional equalities are valid. Do the same for\/ $\Sigma$-types.
\end{exr}

\begin{solution}
    Define
    \[
        \fun{rec}_{A\times B}\colon
            \prod_{C\colon\univ}(A\to B\to C)
            \to
            A\times B\to C
    \]
    specified as
    \[
        \fun{rec}_{A\times B}(C,g)
            \defeq\lambda x.\,g(\pi_1(x))(\pi_2(x)).
    \]
    Given $x\colon A\times B$ we have $\pi_1(x)\colon A$ and $\pi_2(x)\colon B$. It follows that $g(\pi_1(x))\colon B\to C$ and so $g(\pi_1(x))(\pi_2(x))\colon C$. Hence, the function is well defined. Moreover, if we evaluate the function at a canonical element $(a,b)$, since $\pi_1((a,b))\equiv a$ and $\pi_2((a,b))\equiv b$, we obtain
    \[
        g(\pi_1(a,b))(\pi_2((a,b)))\equiv g(a)(b).
    \]
    Then,
    \[
        \fun{rec}_{A\times B}(C,g)((a,b))
            \equiv g(a)(b),
    \]
    which is the definition of the recursor function (cf.~\nref{lpar:products-revisited}).

    For $\Sigma$-types, we define
    \[
        \fun{rec}_{\sum_{x\colon A} B(x)}\colon
            \prod_{C\colon\univ}\Bigl(\prod_{x\colon A}\bigl(B(x)
                \to C\bigr)\Bigr)
                \to \Bigl(\sum_{x\colon A}B(x)\Bigr)
                \to C
    \]
    and specify it as
    \[
        \fun{rec}_{\sum_{x\colon A}B(x)}
            \defeq\lambda C.\,\lambda g.\,
                \lambda p.\,g(\pi_1(p))(\pi_2(p)).
    \]
    To see that this is well defined, let us first observe that $g$ can be applied to $\pi_1(p)$. We have
    \[
        g\colon\prod_{x\colon A}\bigl(B(x)\to C\bigr).
    \]
    The signature of $\pi_1$ implies that $\pi_1(p)\colon A$. Hence, $g(\pi_1(p))\colon B(\pi_1(p))\to C$.

    Second, since the signature of $\pi_2$ guarantees that $\pi_2(p)\colon B(\pi_1(p))$, we also see that $g(\pi_1(p))$ can be applied to $\pi_2(p)$ to get an element of $C$.

    It remains to be seen that the recursion works as expected. To verify this we have to evaluate the function at canonical elements. In this case it suffices to compute the $\lambda$-expression when $p\equiv(a,b)$ with $b\colon B(a)$. Here we have
    \begin{align*}
        \fun{rec}_{\sum_{x\colon A}B(x)}(C,g)((a,b))
            &\equiv\bigl(\lambda p.\,g(\pi_1(p))(\pi_2(p))\bigr)((a,b))\\
            &\equiv g(\pi_1((a,b))(\pi_2((a,b)))\\
            &\equiv g(a)(b),
    \end{align*}
    because $\pi_1((a,b))\equiv a$ and $\pi_2((a,b))\equiv b$. Since this coincides with \eqref{eq:Sigma-type-rec-evaluation}, our definition of $\fun{rec}$ recovers the definition given in \nref{lpar:Sigma-type-rec-and-ind}.
\end{solution}

\begin{exr}
    Assuming as given only the iterator for natural numbers
    \[
        \fun{iter} \colon \prod_{C\colon\univ} C \to (C \to C) \to \N \to C
    \]
    with the defining equations
    \begin{align*}
        \fun{iter}(C, c_0, c_s, 0) &\defeq c_0,\\
        \fun{iter}(C, c_0, c_s, \fun{succ}(n)) &\defeq c_s(\fun{iter}(C, c_0, c_s, n)),
    \end{align*}
    derive a function having the type of the recursor\/ $\fun{rec}_\N$. Show that the defining equations of the recursor hold propositionally for this function, using the induction principle for\/ $\N$.
\end{exr}

\begin{solution}
    Fix $C\colon\univ$. We have to define
    \[
        \fun{rec}\colon\prod_{C\colon\univ}C\to(\N\to C\to C)\to\N\to C
    \]
    using $\fun{iter}$. Suppose we are given $c_0\colon C$ and $c_s\colon\N\to C\to C$. We have to produce a function $\fun{rec}(C,c_0,c_s)\colon\N\to C$ such that
    \begin{align*}
        \fun{rec}(C,c_0,c_s,0)&\defeq c_0,\\
        \fun{rec}(C,c_0,c_s,\fun{succ}(n))
            &\defeq c_s(n,\fun{rec}(C,c_0,c_s,n)).
    \end{align*}
    Let us first try to derive the function $\fun{pred}\colon\N\to\N$ from $0\colon\N$, $\fun{succ}\colon\N\to\N$ and $\fun{iter}$. For consider the case where $C$ is $\N\times\N$. We have to define a function
    \[
        p_s\colon(\N\times\N)\to(\N\times\N)
    \]
    and an element $p_0\colon\N\times\N$. Invoking $\fun{iter}$, with these two pieces of data we obtain a function $\fun q\colon\N\to(\N\times\N)$, given by $\lambda n.\,\fun{iter}(\N\times\N,p_0,p_s,n)$ which satisfies
    \begin{align*}
        \fun q(0)&\defeq p_0\\
        \fun q(\fun{succ}(n)) &\defeq p_s(\fun q(n)).
    \end{align*}
    Let $p_0\defeq(0,0)$ and $p_s(n,m)\defeq(\fun{succ}(n),n)$. Using the projections we can write $\fun q(n)\equiv(q_1(n),q_2(n))$, where $q_1,q_2\colon\N\to\N$. Therefore,
    \begin{align*}
        q_1(0)&\equiv 0\\
        q_2(0)&\equiv 0\\
        q_1(\fun{succ}(n))&\equiv \fun{succ}(q_1(n))\\
        q_2(\fun{succ}(n))&\equiv q_1(n).
    \end{align*}
    These equations suggest that $q_1$ is the identity and $q_2$ is $\fun{pred}$. However, as we will see below, such equalities are propositional.
    
    To define $\fun{rec}$, we will apply $\fun{iter}$ to $\hat C\defeq\N\to C$. For $\hat c_0$ we take the constant function $\lambda n.\,c_0$. For $\hat c_s$
    \begin{align*}
        \hat c_s&\colon(\N\to C) \to (\N\to C)\\
        \hat c_s(\theta)&\defeq\lambda n.\,c_s(n)(\theta(q_2(n))).
    \end{align*}
    This is well defined: given $n\colon\N$, we have $c_s(n)\colon C\to C$ and $\theta(q_2(n))\colon C$, and so $c_s(n)(\theta(q_2(n)))\colon C$.

    Now we can define
    \begin{align*}
        \fun{rec}(C,c_0,c_s,n)
            &\defeq \fun{iter}(\hat C,\hat c_0,\hat c_s,q_1(n))
                (q_2(q_1(n))),
    \end{align*}
    which satisfies
    \begin{align*}
        \fun{rec}(C,c_0,c_s,0) &\equiv\hat c_0(0)\\
            &\equiv(\lambda n.\,c_0)(0)\\
            &\equiv c_0.
    \intertext{and}
        \fun{rec}(C,c_0,c_s,\fun{succ}(n))
            &\equiv \fun{iter}(\hat C,\hat c_0,\hat c_s,
                q_1(\fun{succ}(n)))(q_2(q_1(\fun{succ}(n)))\\
            &\equiv \fun{iter}\bigl(\hat C,\hat c_0,\hat c_s,
                \fun{succ}(q_1(n))\bigr)(q_1(q_1(n)))\\
            &\equiv\hat c_s\bigl(\fun{iter}(\hat C,\hat c_0,\hat c_s,q_1(n))\bigr)
                (q_1(q_1(n)))\\
            &\equiv c_s\bigl(q_1(n)\bigr)\bigl(\fun{iter}(\hat C,\hat c_0,\hat c_s,q_1(n))\bigr)
                (q_2(q_1(q_1(n))))\\
            &\equiv c_s\bigl(q_1(n)\bigr)
                \bigl(\fun{rec}(C,c_0,c_s,q_1(n))\bigr)\\
            &\equiv c_s\bigl(q_1(n),\fun{rec}(C,c_0,c_s,q_1(n))\bigr).
    \end{align*}
    Thus, the question of proving that $\fun{rec}$ is propositionally equal to the recursion principle $\fun{rec}_\N$ for $\N$, would follow from the fact that a function $f\colon\N\to\N$ that satisfies $f(0)\equiv0$ and $f(\fun{succ}(n))\equiv\fun{succ}(n)$ is propositionally equal to the identity, i.e., $f\eq{\N\to\N}\id_\N$.

    However, strictly speaking, that would be too much for our needs, since the equality $q_1(n)=n$ for $n\colon\N$ would imply by congruence\footnote{See Exercise~\ref{exr:ap-application}.} that $\fun{rec}$ satisfies the recursion principle equations propositionally, which is what we want.

    In other words, we need a witness of
    \[
        \prod_{n\colon\N} q_1(n) \eq\N n.
    \]
    To construct it, we use the induction principle \nref{lpar:induction} for the type family
    \[
        C\colon\N \to \univ, \quad C(n) \defeq q_1(n) \eq\N n,
    \]
    which requires a constant $c_0\colon C(0)$ and a step function 
    \[
        c_s\colon\prod_{n\colon\N}C(n)\to C(\fun{succ}(n)).
    \]
    The induction principle yields
    \[
        \fun{ind}_\N(C,c_0,c_s)\colon\prod_{n\colon\N}C(n).
    \]
    For $c_0$, we define it as $\fun{refl}_0$ because $q_1(0)\equiv 0$. For the inductive step, given $n\colon\N$ and a hypothesis $h\colon C(n)$ (i.e., $h\colon q_1(n)\eq\N n$), we define $c_s(n,h)$ by applying $\fun{succ}$ to $h$ via congruence. This provides a witness for $q_1(\fun{succ}(n))\eq\N\fun{succ}(n)$, since $q_1(\fun{succ}(n))\equiv\fun{succ}(q_1(n))$.
    
\end{solution}


\begin{exr}
    Show that if we define\/ $A+B\defeq\sum_{x\colon\fun2}\fun{rec}_{\fun2}(\univ,A,B,x)$, then we can give a definition of\/ $\fun{ind}_{A+B}$ for which the definitional equalities stated in {\upshape\nref{lpar:coproduct-induction}} hold.
\end{exr}

\begin{solution}
    This is a complement to \nref{lpar:coproduct-as-disjoint-union}, where the notation for the coproduct is $A\oplus B$. The induction function we have to define is
    \[
        \fun{ind}_{A\oplus B}\colon\prod_{C\colon A\oplus B\to\univ}\;
            \prod_{x\colon A}C(\fun{in}_A(x))
            \to
            \prod_{y\colon B}C(\fun{in}_B(y))
            \to
            \prod_{s\colon A\oplus B}C(s).
    \]
    Fix $C\colon A\oplus B\to\univ$. Given two dependent functions
    \[
        g_0\colon\prod_{x\colon A}C(\fun{in}_A(x))
        \quad\text{and}\quad
        g_1\colon\prod_{y\colon B}C(\fun{in}_B(y)),
    \]
    we have to define a function with signature
    \[
        f\colon\prod_{s\colon A\oplus B} C(s).
    \]
    To do this we must use the induction principle of the $\Sigma$-type, which requires (see \nref{lpar:Sigma-type-rec-and-ind}) constructing a function $g$ with the following signature 
    \[
        g\colon
            \prod_{t\colon\fun2\vphantom{\univ y}}\;
            \prod_{z\colon\fun{rec}_{\fun2}(\univ,A,B,t)}
            C((t,z)).
    \]
    As a result, we will obtain
    \[
        f \defeq\fun{ind}_{\sum_{t\colon\fun2}\fun{rec}_{\fun2}(\univ,A,B,t)}
            (C,g).
    \]
    Now, by \nref{lpar:boolean-type}, we have
    \begin{align*}
        \fun{rec}_{\fun2}(\univ,A,B)\colon\fun2&\to\univ\\
        0_{\fun2}&\mapsto A\\
        1_{\fun2}&\mapsto B.
    \end{align*}
    Therefore, by \nref{lpar:boolean-type}, we can define $g$ as
    \[
        g\defeq\lambda t\colon\fun2.\,\fun{ind}_{\fun2}(D,g_0,g_1,t).
    \]
    where $D\colon\fun2\to\univ$ is given by
    \[
        D(t)\defeq\prod_{z\colon\fun{rec}_{\fun2}(\univ,A,B,t)}C((t,z)),
    \]
    which, according to \eqref{eq:inA-inB}, confirms the required typing judgments $g_0\colon D(0_{\fun2})$ and $g_1\colon D(1_{\fun2})$.
    %
    
\end{solution}

\begin{exr}${}$
    \begin{enumerate}[a),font=\upshape]
        \item Define multiplication and exponentiation using\/ $\fun{rec}_\N$.

        \item Verify that\/ $(\N,+,0,{}\cdot{},1)$ is a semiring using only\/ $\fun{ind}_\N$.

        \item Verify that $m^{n+n'}\eq\N m^n\cdot m^{n'}$.
    \end{enumerate}
\end{exr}

\begin{solution}
    \begin{enumerate}[a)]
        \item Multiplication was defined in \nref{lpar:natural-multiplication}.

        For exponentiation, we take\/ $C\defeq\N\to\N$ and define
        \begin{align*}
            c_0 &\colon \N\to\N \\
            c_0 &\defeq \lambda m.\,1,\\
            c_s &\colon \N\to(\N\to\N)\to(\N\to\N) \\
            c_s &\defeq \lambda n.\,\lambda g\colon\N\to\N.\,\lambda m.\,g(m)\cdot m.
        \end{align*}
        From \eqref{tik:natural-rec}, we obtain
        \[
            \fun{rec}_\N(C,c_0,c_s)\colon\N\to(\N\to\N).
        \]
        We define exponentiation as
        \[
            \fun{exp}\defeq\lambda n.\,\fun{rec}_\N(C,c_0,c_s,n).
        \]
        This definition satisfies
        \begin{align*}
            \fun{exp}(0,m)
                &\equiv \fun{rec}_\N(C,c_0,c_s,0)(m) \\
                &\equiv c_0(m)\\
                &\equiv 1, \\[1ex]
            \fun{exp}(\fun{succ}(n),m)
                &\equiv \fun{rec}_\N(C,c_0,c_s,\fun{succ}(n))(m)\\
                &\equiv c_s(n,\fun{rec}_\N(C,c_0,c_s,n))(m)\\
                &\equiv \fun{rec}_\N(C,c_0,c_s,n)(m)\cdot m\\
                &\equiv \fun{exp}(n,m)\cdot m.
        \end{align*}

        \item From \nref{lpar:additive-identity} and \nref{lpar:associativity-of-addition}, we know that $0$ is additive identity of $\N$ and that addition is associative. Therefore, we have to prove the following propositions\footnote{For commutativity of addition see Exercise~\ref{exr:+comm}.}
        \begin{enumerate}[1.]
            \item \textit{Annihilation.} Since $0\cdot n\equiv n$, it suffices to show that $n\cdot0\eq\N n$ by induction on $n\colon\N$.

            The base case is trivial because $0\cdot0\equiv0$ by definition.
            
            For the inductive step, take $p\colon n\cdot0\eq\N0$. We have to see that $\fun{succ}(n)\cdot0\eq\N0$ is inhabited. Since $\fun{succ}(n)\cdot0\equiv n\cdot0+0$, the conclusion follows from path concatenation: take $q\colon n\cdot0+0\eq\N n\cdot0$ and obtain $q\ct p\colon\fun{succ}(n)\cdot0\eq\N0$.

            \item \textit{Left distributivity.} Define the motive $C\colon\N\to\univ$ by
            \[
                D(i)\defeq\prod_{j,k\colon\N}i\cdot(j+k)\eq\N i\cdot j+i\cdot k.
            \]
            Then
            \[
                D(0)\equiv\prod_{j,k\colon\N}0\cdot(i+j)\eq\N0\cdot i+0\cdot j,
            \]
            which is judgmentally equal to $\prod_{i,j\colon\N}0\eq\N0$ because $0\equiv0+0$. Thus, the constant map $d_0\colon\N\times\N\to\fun{refl}_0$ is a witness.

            For the inductive step take $p\colon D(i)$ and observe that
            \begin{align*}
                \fun{succ}(i)\cdot(j+k) &\equiv i\cdot(j+k)+(j+k)\\
                    &\eq\N (i\cdot j+i\cdot k)+(j+k)
                        &&\text{; by }p\\
                    &\eq\N (i\cdot j+j)+(i\cdot k+k)
                        &&\text{; +assoc \& +comm}\\
                    &\eq\N \fun{succ}(i)\cdot j+\fun{succ}(i)\cdot k,
            \end{align*}
            where we have used congruence\footnote{Exercise~\ref{exr:ap-application}.} in obvious ways. The conclusion follows from \nref{lpar:path-concatenation}.

            \item \textit{Multiplicative identity.} First observe that
            \[
                1\cdot n\equiv\fun{succ}(0)\cdot n\equiv 0\cdot n+n
                    \equiv 0+n\equiv n.
            \]
            To prove $n\cdot1\eq\N n$, we proceed by induction on $n$. The case $n\equiv0$ is trivial: $n\cdot1\equiv0\cdot1\equiv 0\equiv n$.
            Suppose $p\colon n\cdot1\eq\N n$. Then
            \begin{align*}
                \fun{succ}(n)\cdot1&\equiv n\cdot1+1\\
                    &\eq\N n+1
                        &&\text{; $p$ + congruence}\\
                    &\eq\N 1+n
                        &&\text{; commutativity}\\
                    &\equiv\fun{succ}(n).
            \end{align*}

            \item \textit{Multiplication commutativity.} Define the motive $C\colon\N\to\univ$ by
            \[
                C(i)\defeq\prod_{j\colon\N}i\cdot j\eq\N j\cdot i.
            \]
            The base case, where $i\defeq0$, follows from annihilation (part 1). 

            For the inductive step, take $p\colon i\cdot j\eq\N j\cdot i$. We have to show that $\fun{succ}(i)\cdot j\eq\N j\cdot\fun{succ}(i)$. Using $\fun{succ}(i)\cdot j\equiv i\cdot j+j$ and left distributivity (part 2), we obtain
            \begin{align*}
                i\cdot j+j &\eq\N j\cdot i+j
                        &&\text{; by }p\\
                    &\eq\N j\cdot1+j\cdot i
                        &&\text{; +commutativity \& mult.~identity}\\
                    &\eq\N j\cdot(1+i)
                        &&\text{; left distributivity}\\
                    &\equiv j\cdot\fun{succ}(i),
            \end{align*}
            and the conclusion follows from path concatenation.
            
            \item \text{Right distributivity.} Directly follows from commutativity (part~4) and left distributivity (part~2).
            
            \item \textit{Multiplication associativity.} Define the motive $A\colon\N\to\univ$ given by
            \[
                A(i)\defeq\prod_{j,k\colon\N}(i\cdot j)\cdot k \eq\N i\cdot(j\cdot k).
            \]
            The base case is
            \[
                A(0)\equiv\prod_{j,k\colon\N}
                    (0\cdot j)\cdot k\eq\N 0\cdot(j\cdot k),
            \]
            which is trivial because, by definition, $0\cdot n\equiv n$ for $n\colon\N$.

            In the inductive step we assume $p\colon A(i)$ and must exhibit a witness of 
            \[
                (\fun{succ}(i)\cdot j)\cdot k
                    \eq\N\fun{succ}(i)\cdot(j\cdot k).
            \]
            For
            \begin{align*}
                (\fun{succ}(i)\cdot j)\cdot k
                    &\equiv (i\cdot j+j)\cdot k\\
                    &\eq\N (i\cdot j)\cdot k+j\cdot k
                        &&\text{; right distributivity}\\
                    &\eq\N i\cdot(j\cdot k)+j\cdot k
                        &&\text{; by }p\\
                    &\equiv\fun{succ}(i)\cdot(j\cdot k).
            \end{align*}
        \end{enumerate}

        \item Consider the motive $E\colon\N\to\univ$ given by
        \[
            E\defeq\prod_{n,m,n'\colon\N}m^{n+n'}\eq\N m^n\cdot m^{n'}.
        \]
        Fix $m$ and $n'$. To construct a witness of $E(m,n')$ we proceed by induction on $n$.
        For the base case, we have
        \begin{align*}
            m^{0+n'}&\equiv m^{n'} &&;\ \eqref{eq:0+n-equiv-n}\\
                &\equiv1\cdot m^{n'}\\
                &\equiv m^0\cdot m^{n'} &&;\ \text{part a)}
        \end{align*}
        Suppose that $p\colon E(m,n')(n)$. Fix $m,n'\colon\N$ and put
        \[
            q\defeq p(m,n')\colon m^{n+n'}\eq\N m^n\cdot m^{n'}.
        \]
        We have to construct a path for
        \[
            m^{\fun{succ}(n)+n'}\eq\N m^{\fun{succ}(n)}\cdot m^{n'}.
        \]
        Now
        \begin{align*}
            m^{\fun{succ}(n)+n'} &\equiv m^{\fun{succ}(n+n')}\\
                &\equiv m^{n+n'}\cdot m\\
                &\eq\N(m^n\cdot m^{n'})\cdot m
                    &&\text{; by $q$ + congruence}\\
                &\eq\N(m^n\cdot m)\cdot m^{n'}
                    &&\text{; associativity \& commutativity}\\
                &\eq\N m^{\fun{succ}(n)}\cdot m^{n'}.
        \end{align*}
    \end{enumerate}
\end{solution}

\begin{exr}\label{exr:Fin}
    Define the type family\/ $\fun{Fin}\colon\N\to\univ$ mentioned at the end of {\upshape\nref{lpar:type-families}}, and the dependent function\/ $\fun{fmax}\colon\prod_{n\colon\N}\fun{Fin}(\fun{succ}(n))$ mentioned in {\upshape\nref{lpar:Pi-types}}.
\end{exr}

\begin{solution}
    The $\fun{Fin}$ type family requires a motive~$C$ such that $\fun{Fin}(n)\colon C(n)$. Since $\fun{Fin}(n)$ is a type, $C$ must be $\univ$.

    To define $\fun{Fin}$ using the recursion principle, we specify $c_0\colon\univ$ and $c_s\colon\N\to\univ\to\univ$ as follows
    \begin{align*}
        c_0&\defeq\fun0,
                &&\text{; empty coproduct}\\
        c_s&\defeq\lambda n.\,\lambda A.\,A+\fun1.
                &&\text{; $A$ + empty product}
    \end{align*}
    As a result, we can define $\fun{Fin}\defeq\fun{rec}_\N(\univ,c_0,c_s)\colon\N\to\univ$, which satisfies
    \[
        \fun{Fin}(0)\defeq\fun0
        \quad\text{and}\quad
        \fun{Fin}(\fun{succ}(n))\equiv\fun{Fin}(n)+\fun1.
    \]
     To define $\fun{fmax}$ we simply declare
    \begin{align*}
        \fun{fmax}\colon\N&\to\fun{Fin}(n)+\fun1\\
        n&\mapsto\fun{inr}_n(\fun\star),
    \end{align*}
    where $\fun{inr}_n$ denotes the $n$th right inclusion of $\fun1$ into $\fun{Fin}(n)+\fun1$. This completes the proof because $\fun{Fin}(n)+\fun1\equiv\fun{Fin}(\fun{succ}(n))$.
    %
    
\end{solution}

\begin{exr}
    Show that the \textsl{Ackermann} function\/ $\fun{ack}\colon\N\to\N\to\N$ is definable using only\/ $\fun{rec}_\N$ satisfying the following equations:
    \begin{align*}
        \fun{ack}(0,n)&\equiv\fun{succ}(n),\\
        \fun{ack}(\fun{succ}(m),0)
            &\equiv\fun{ack}(m,1),\\
        \fun{ack}(\fun{succ}(m),\fun{succ}(n))
            &\equiv\fun{ack}(m,\fun{ack}(\fun{succ}(m),n)).
    \end{align*}
\end{exr}

\begin{solution}
    Take the motive $C\defeq\N\to\N$. To use the $\fun{rec}_\N$ function we need to define
    \begin{align*}
        c_0\colon\N\to\N,
        \quad\text{and}\quad
        c_s\colon\N\to(\N\to\N)\to(\N\to\N).
    \end{align*}
    According with equations \eqref{eq:recursion-semantics} for the case where $f$ is $\fun{ack}$, by defining
    \[
        \fun{ack}(m,{}\cdot{})\defeq\fun{rec}_\N(C,c_0,c_s,m)
    \]
    we obtain
    \begin{align*}
        \fun{ack}(0,n)&\equiv c_0(n)\\
        \fun{ack}(\fun{succ}(m),{}\cdot{})
            &\equiv c_s(m,\fun{ack}(m,{}\cdot{}))
    \end{align*}
    The equation $\fun{ack}(0,n)\equiv\fun{succ}(n)$, that we must honor, makes the definition of~$c_0$ straightforward: $c_0\defeq\fun{succ}$. 
    
    To define $c_s(m,g)\colon\N\to\N$, we fix $m\colon\N$ and $g\colon\N\to\N$, with $g$ playing the role of $\fun{ack}(m,{}\cdot{})$, and proceed by recursion on $n$. We need to provide two pieces of information
    \begin{align*}
        d_0&\colon\N\\
        d_s&\colon\N\to\N\to\N,
    \end{align*}
    where $d_0\equiv c_s(m,g)(0)$. Since $c_s$ is the step function on $m$ for $\fun{ack}(m,{}\cdot{})$, we deduce that
    \[
        d_0 \equiv \fun{ack}(\fun{succ}(m),0)\equiv\fun{ack}(m,1).
    \]
    Therefore, we must specify $d_0\defeq g(1)$.

    Since $c_s(m,g)$ represents $\fun{ack}(\fun{succ}(m),{}\cdot{})$, and
    \[
        d_s(n,c_s(m,g)(n))\equiv c_s(m,g)(\fun{succ}(n)),
    \]
    we deduce that $d_s(n,c_s(m,g)(n))$ must be a representative of
    \[
        \fun{ack}(\fun{succ}(m),\fun{succ}(n))
            \equiv\fun{ack}(m,\fun{ack}(\fun{succ}(m),n)).
    \]
    Therefore, if we define
    \begin{equation}\label{eq:ds-definition}
        d_s\defeq\lambda n,r.\, g(r),
    \end{equation}
    we obtain
    \begin{equation}\label{eq:cs-from-ds}
        c_s \defeq \lambda m,g.\, \fun{rec}_\N(\N, g(1), d_s).
    \end{equation}
    Now we can define the full function $\fun{ack}$ using the outer recursion on $m$:
    \[
        \fun{ack} \defeq \fun{rec}_\N(\N\to\N, \fun{succ}, c_s).
    \]
    Let us verify the three properties of the Ackermann function.
    \begin{enumerate}[$\mbf\to$]
        \item $\fun{ack}(0, n) \equiv \fun{succ}(n)$
        \begin{align*}
            \fun{ack}(0, n)
                &\equiv \fun{rec}_\N(\N\to\N, c_0, c_s, 0)(n) \\
                &\equiv c_0(n) \\
                &\equiv \fun{succ}(n).
        \end{align*}

    \item $\fun{ack}(\fun{succ}(m), 0) \equiv \fun{ack}(m, 1)$
    \begin{align*}
        \fun{ack}(\fun{succ}(m), 0)
            &\equiv \fun{rec}_\N(\N\to\N, c_0, c_s, \fun{succ}(m))(0) \\
            &\equiv c_s(m, \fun{ack}(m))(0) \\
            &\equiv \bigl(\lambda g.\,\fun{rec}_\N(\N, g(1),d_s)\bigr)(\fun{ack}(m))(0)
                &&\text{; \eqref{eq:cs-from-ds}}\\
            &\equiv \fun{rec}_\N(\N, \fun{ack}(m)(1), d_s, 0) \\
            &\equiv \fun{ack}(m)(1) \\
            &\equiv \fun{ack}(m, 1).
    \end{align*}

    \item $\fun{ack}(\fun{succ}(m),\fun{succ}(n))\equiv\fun{ack}(m,\fun{ack}(\fun{succ}(m),n))$
    \begin{align*}
        \fun{ack}(\fun{succ}(m),\fun{succ}(n))
            &\equiv c_s(m,\fun{ack}(m))(\fun{succ}(n))\\
            &\equiv\fun{rec}_\N(\N,\fun{ack}(m)(1),d_s,\fun{succ}(n))\\
            &\equiv d_s\bigl(n,\fun{rec}_\N(\N,\fun{ack}(m)(1),d_s,n)\bigr)\\
            &\equiv d_s\bigl(n,c_s(m,\fun{ack}(m))(n)\bigr)
                &&\text{; \eqref{eq:cs-from-ds}}
    \intertext{\small(at this point the variable $g$ of \eqref{eq:ds-definition} is bound to $\fun{ack}(m)$; consequently)}
            &\equiv d_s\bigl(n,\fun{ack}(\fun{succ}(m),n)\bigr)
                &&\text{; recursion}\\
            &\equiv\fun{ack}(m)\bigl(\fun{ack}(\fun{succ}(m),n)\bigr)
                &&\text{; \eqref{eq:ds-definition}}\\
            &\equiv \fun{ack}(m, \fun{ack}(\fun{succ}(m), n)).
    \end{align*}
        \end{enumerate}

\end{solution}


\begin{exr}
    Show that for any type\/ $A$, we have\/ $\neg\neg\neg A \to \neg A$.
\end{exr}

\begin{solution}
    Recall that $\neg A\defeq A\to\fun0$. So, we have to find a function
    \[
        f\colon (((A\to\fun0)\to\fun0)\to\fun0)\to(A\to\fun0).
    \]
    Fix $Z\colon\univ$ and $a\colon A$ and consider the evaluation maps
    \begin{align*}
        \ev a\colon(A\to Z)&\to Z\\
        x&\mapsto x(a).
    \intertext{and}
        \ev{\ev a}\colon((A\to Z)\to Z)&\to Z\\
        y&\mapsto y(\ev a).
    \end{align*}
    Then define
    \begin{align*}
        \ev{\ev{\ev a}}\colon(((A\to Z)\to Z)\to Z)\to A&\to Z\\
            (z,a)&\mapsto z(\ev{\ev a}).
    \end{align*}
    To conclude the proof, take $Z\defeq\fun0$.
\end{solution}

\begin{exr}
    Using the propositions as types interpretation, derive the following tautologies.
    \begin{enumerate}[a), font=\upshape]
        \item If\/ $A$, then (if\/ $B$ then\/ $A$).
        \item If\/ $A$, then not (not\/ $A$).
        \item If (not\/ $A$ or not\/ $B$), then not ($A$ and\/ $B$).
    \end{enumerate}
\end{exr}

\begin{solution}
    \begin{enumerate}[a)]
        \item This translates into the type $A\to(B\to A)$. An obvious inhabitant is $a\mapsto(b\mapsto a)$.

        \item This corresponds to $A\to((A\to\fun0)\to\fun0)$. A function of this type is a particular case of
        \begin{align*}
            \ev a\colon A&\to((A\to Z)\to Z)\\
            a&\mapsto(g\mapsto g(a)).
        \end{align*}
        when $Z\defeq\fun0$.

        \item Here the interpretation is $\neg A+\neg B\to\neg (A\times B)$.
        \begin{align*}
            (A\to\fun0)+(B\to\fun0)&\to(A\times B)\to\fun0.
        \end{align*}
        Therefore, it suffices to define
        \begin{align*}
            g_0\colon(A\to\fun0)&\to(A\times B\to\fun0)\\
            g_1\colon(B\to\fun0)&\to(A\times B\to\fun0)
        \end{align*}
        which can be done respectively as
        \begin{align*}
            x&\mapsto((a,b)\mapsto x(a))\\
            y&\mapsto((a,b)\mapsto y(b))
        \end{align*}
    \end{enumerate}
\end{solution}

\begin{exr}
    Using propositions-as-types, derive the double negation of the principle of excluded middle, i.e.\/ prove\/ $\neg\neg(P\lor\neg P)$.    
\end{exr}

\begin{solution}
    We have to define a function
    \[
        \bigl(P+(P\to\fun0)\to\fun0\bigr)\to\fun0.
    \]
    More generally, let us define a function
    \[
        f\colon\bigl(P+(P\to Z)\to Z\bigr)\to Z.
    \]
    Fix a function $g\colon P+(P\to Z)\to Z$. It defines two functions
    \begin{align*}
        g_0 &\colon P \to Z & g_1 &\colon (P \to Z) \to Z \\
        g_0 &\defeq \lambda p.\, g(\fun{inl}(p)) & g_1 &\defeq \lambda h.\, g(\fun{inr}(h)).
    \end{align*}
    Then define
    \[
        f \defeq \lambda g.\, g_1(g_0).
    \]
\end{solution}


\begin{exr}
    Explain why the induction principles for identity types do not allow us to construct a function
    \[
        f\colon\prod_{x\colon A}\;\prod_{p\colon x=x} (p=\fun{refl}_x)
    \]
    with the defining equation
    \[
        f(x,\fun{refl}_x)\defeq\fun{refl}_{\fun{refl}_x}.
    \]
\end{exr}

\begin{solution}
    The construction is invalid because the expression does not fit the type signature required by the induction principle for identity types in either formulation \eqref{eq:ind=A} or \eqref{eq:ind'=A}.

    The given expression only considers paths of the form $p\colon x=x$, between $x$ and itself, while the induction principle requires paths between two variables.
\end{solution}

\begin{exr}\label{exr:+comm}
    Show that addition of natural numbers is commutative.
\end{exr}

\begin{solution}
    We must exhibit a witness for the type
    \[
        \prod_{i,j\colon\N} i+j \eq\N j+i.
    \]
    We proceed by induction on $i$. Define the motive $C \colon \N \to \univ$ by
    \[
        C(i) \defeq \prod_{j\colon\N} i+j \eq\N j+i.
    \]
    
    \textbf{Base case:} We must provide a term $c_0 \colon \prod_{j\colon\N}0+j\eq\N j+0$.
    By definition of addition, $0+j \equiv j$. Thus, the goal reduces to showing $j \eq\N j+0$. We have already constructed a witness for this in \nref{lpar:additive-identity}.

    \textbf{Inductive step:} Let $i\colon\N$ and assume the inductive hypothesis
    \[
        h \colon \prod_{j\colon\N} i+j \eq\N j+i.
    \]
    We must show that for any $j$, $\fun{succ}(i)+j \eq\N j+\fun{succ}(i)$.
    \begin{align*}
        \fun{succ}(i)+j &\equiv \fun{succ}(i+j) \\
            &\eq\N \fun{succ}(j+i) 
                &&\text{; $h(j)$ and congruence}\\
            &\eq\N (j+i)+1 
                &&\text{; } \fun{succ}(n) \eq\N n+1\\
            &\eq\N j+(i+1) 
                &&\text{; associativity}\\
            &\eq\N j+\fun{succ}(i) 
                &&\text{; } n+1 \eq\N \fun{succ}(n).
    \end{align*}
\end{solution}

\section{Additional Exercises}

\begin{exr}\label{exr:ap-application}
    In {\upshape\nref{lpar:associativity-of-addition}}, we relied on the \textsl{principle of congruence} (applying a function to both sides of an equality).
    \begin{enumerate}[a), font=\upshape]
        \item Define the \textsl{application}\footnote{ A.k.a.~\textsl{action on paths}.} function\/ $\fun{ap}_f$, which takes a function\/ $f\colon A\to B$ and a path\/ $p\colon x\eq Ay$ and produces a path\/ $f(x)\eq Bf(y)$.
        
        \item Show that\/ $\fun{ap}_f$ distributes over path concatenation, i.e.,
        \[
            \fun{ap}_f(p\ct q)\eq {f(x)\eq Bf(z)}\fun{ap}_f(p)\ct \fun{ap}_f(q),
        \]
        for\/ $x,y,z\colon A$, $p\colon x\eq Ay$, and\/ $q\colon y\eq Az$.
    \end{enumerate}
\end{exr}

\needspace{2\baselineskip}
\begin{solution}
    \begin{enumerate}[a)]
        \item Fix the types $A$ and $B$, and a function $f\colon A\to B$. Consider the motive family
        \begin{align*}
            C_f&\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ\\
            C_f&\defeq\lambda x.\,\lambda y.\,\lambda p.\,f(x)\eq Bf(y).
        \end{align*}
        To apply the induction principle, we need a base case function $c_f$ of type
        \[
             \prod_{x\colon A} C_f(x,x,\fun{refl}_x).
        \]
        Since $C_f(x,x,\fun{refl}_x) \equiv f(x)\eq B f(x)$, we can define this function as
        \[
            c_f \defeq \lambda x.\,\fun{refl}_{f(x)}.
        \]
        Then, applying the induction principle $\fun{ind}_{\eq A}$  to $C_f$ and $c_f$, as specified in \eqref{eq:ind=A} yields a function
        \begin{align*}
            \fun{ind}_{\eq A}(C_f,c_f)\colon
                \prod_{x,y\colon A}\;\prod_{p\colon x\eq Ay}f(x)\eq Bf(y),
        \end{align*}
        which satisfies
        \[
            \fun{ind}_{\eq A}(C_f,c_f,x,x,\fun{refl}_x)
            \equiv c_f(x)
            \equiv \fun{refl}_{f(x)}.
        \]
        Hence, we can introduce the \textsl{action of paths} function as
        \[
            \fun{ap}_f\defeq\fun{ind}_{\eq A}(C_f,c_f).
        \]
        The complete notation $\fun{ap}_f(x,y,p)$ is usually simplified to $\fun{ap}_f(p)$, or even to~$\tilde f(p)$. In particular, 
        \[
            \tilde f(\fun{refl}_x)\equiv \fun{refl}_{f(x)}.
        \]
    
        \item Fix $f\colon A\to B$ and consider the motive
        \begin{align*}
            D_f&\colon\prod_{x,y\colon A}(x\eq Ay)\to\univ\\
                D_f(x,y,p)&\defeq\prod_{z\colon A}\;\prod_{q\colon y\eq Az}
                \tilde f(p\ct q)\eq {f(x)\eq Bf(z)}
                    \tilde f(p)\ct\tilde f(q).
        \end{align*}
        Using part a) and the fact that $\fun{refl}$ is the  definitionally left concatenation unit, we deduce
        \begin{align*}
            D_f(x,x,\fun{refl}_x)
                &\equiv\prod_{z\colon A}\;\prod_{q\colon x\eq Az}
                    \tilde f(\fun{refl}_x\ct q)\eq{f(x)\eq Bf(x)}
                        \tilde f(\fun{refl}_x)\ct\tilde f(q)\\
                &\equiv \prod_{z\colon A}\;\prod_{q\colon x\eq Az}
                    \tilde f(q)\eq{f(x)\eq Bf(x)}\tilde f(q).
        \end{align*}
        Consequently, we can take
        \[
            d_f(x)\defeq\lambda z.\,\lambda q.\,\fun{refl}_{\tilde f(q)},
        \]
        and apply the induction principle to obtain
        \[
            \fun{ind}_{\eq A}(D_f,d_f,x,y,p)\colon D_f(x,y,p).
        \]\qedhere
    \end{enumerate}
\end{solution}

\begin{exr}
    Consider the boolean type\/ $\fun{2}$.
    \begin{enumerate}[a), font=\upshape]
        \item Define the negation function\/ $\fun{not}\colon\fun2\to\fun2$.
        \item Prove that negation is an involution, i.e., construct a witness for
        \[
            \prod_{b\colon\fun2}\fun{not}(\fun{not}(b))\eq {\fun2} b.
        \]
    \end{enumerate}
\end{exr}

\begin{solution}
    \begin{enumerate}[a)]
        \item It is enough to use the recursion principle \eqref{eq:rec2} for the motive $C\defeq\fun2$. For the constants $c_0,c_1\colon\fun2$, we take
        \[
            c_0\defeq1_{\fun2}
            \quad\textsl{and}\quad
            c_1\defeq0_{\fun2}.
        \]
        Then we can define
        \[
            \fun{not}\colon\fun2\to\fun2,
            \quad
            \fun{not}\defeq\lambda b.\,\fun{rec}_{\fun2}(\fun2,c_0,c_1,b),
        \]
        which satisfies $\fun{not}(0_{\fun2})\equiv1_{\fun2}$ and $\fun{not}(1_{\fun2})\equiv0_{\fun2}$.

        \item Consider the motive $C\colon\fun2\to\univ$ given by
        \[
            C\defeq\prod_{b\colon\fun2}\fun{not}(\fun{not}(b))\eq{\fun2}b.
        \]
        Using the induction principle for $\fun2$, we need to specify two paths $c_0\colon C(0_{\fun2})$ and $c_1\colon C(1_{\fun2})$, which we choose as $\fun{refl}_{0_{\fun2}}$ and $\fun{refl}_{1_{\fun2}}$. These are well defined because 
        \[
            \fun{not}(\fun{not}(0_{\fun2}))
                \equiv\fun{not}(1_{\fun2})
                \equiv0_{\fun2},
        \]
        and similarly for $1_{\fun2}$. As a result, we can define
        \[
            \lambda b.\,\fun{ind}_{\fun2}(C,c_0,c_1,b).
        \]
    \end{enumerate}
\end{solution}

\begin{exr}\label{exr:constant-fibration}
    Prove that transport over a constant type family is the identity function. That is, given a type\/ $B$ and the constant family\/ $P(x)\defeq B$, construct a witness for
    \[
        \prod_{x,y\colon A}\;
            \prod_{p\colon x\eq Ay}\;
            \prod_{b\colon B}\fun{transport}^P(p,b)\eq Bb.
    \]
\end{exr}

\begin{solution}
    The motive $C$ used in \nref{lpar:transport} to derive the transport function $\fun{transport}^P$ is given by
    \[
    C(x,y,p)\defeq P(x)\to P(y),
    \]
    which, in this case, reduces to
    \[
    C(x,y,p)\defeq B\to B,
    \]
    while $c\colon C(x,x,\fun{refl}_x)$ remains the identity $c\defeq\lambda b\colon B.\,b$. Let us also recall that $\fun{transport}^P(x,y,p)\equiv\fun{ind}_{\eq A}(C,c,x,y,p)$.
    
    Consider the motive
    \[
    D(x,y,p)\defeq\prod_{b\colon B}\fun{transport}^P(p,b)\eq B b.
    \]
    For the base case, we need a witness $d$ of type $\prod_{x\colon A} D(x,x,\fun{refl}_x)$.
    Since
    \[
        \fun{transport}^P(x,x,\fun{refl}_x)\equiv\id_B,
    \]
    and $\id_B(b)\equiv b$, evaluating the motive at the base yields
    \[
    D(x,x,\fun{refl}_x) \equiv \prod_{b\colon B}b\eq B b.
    \]
    Hence, we can define $d(x)\defeq\lambda b.\,\fun{refl}_b$.
    We can now invoke the induction principle $\fun{ind}_{\eq A}$ to produce the desired function
    \[
    \fun{ind}_{\eq A}(D,d)\colon\prod_{x,y\colon A}\;\prod_{p\colon x\eq A y} D(x,y,p).
    \]
\end{solution}

\begin{exr}
    Let\/ $f\colon A\to B$ be a function between types\/ $A$ and\/ $B$.
    Show that the action on paths\/ $\fun{ap}_f$ can be defined strictly in terms of the transport function\/ $\fun{transport}$.
    Specifically, given\/ $x,y\colon A$ and a path\/ $p\colon x\eq Ay$, construct a term of type\/ $f(x)\eq Bf(y)$ using only\/ $\fun{transport}$ and\/ $\fun{refl}$.
\end{exr}

\begin{solution}
    Fix\/ $x\colon A$ and consider the family\/ $P\colon A\to\univ$ defined by
    \[
        P(y)\defeq f(x)\eq Bf(y).
    \]
    By transport, we know that there is a function
    \[
        \fun{transport}^P\colon\prod_{y\colon A}\;\prod_{p\colon x\eq Ay}P(x)\to P(y).
    \]
    Since\/ $\fun{refl}_{f(x)}\colon P(x)$, given a path\/ $e\colon x\eq A y$, we can apply the transport function to obtain
    \[
        \fun{transport}^P(y,e)(\fun{refl}_{f(x)})\colon P(y).
    \]
    By definition of\/ $P$, this term is an inhabitant of\/ $f(x)\eq B f(y)$, as required.
\end{solution}

